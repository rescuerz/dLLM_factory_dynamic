#!/usr/bin/env python3
"""
æµ‹è¯•åŠ¨æ€é•¿åº¦è®­ç»ƒé›†æˆ
"""

import sys
import os
sys.path.append('/mnt/40t/zhounan/dLLM_factory_dynamic/sft')

import torch
import logging
from transformers import TrainingArguments

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_dynamic_length_trainer_import():
    """æµ‹è¯•DynamicLengthTraineræ˜¯å¦èƒ½æ­£ç¡®å¯¼å…¥"""
    try:
        from trainer.dynamic_length_trainer import DynamicLengthTrainer
        logger.info("âœ… DynamicLengthTrainerå¯¼å…¥æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"âŒ DynamicLengthTrainerå¯¼å…¥å¤±è´¥: {e}")
        return False

def test_dynamic_length_trainer_init():
    """æµ‹è¯•DynamicLengthTraineræ˜¯å¦èƒ½æ­£ç¡®åˆå§‹åŒ–"""
    try:
        from trainer.dynamic_length_trainer import DynamicLengthTrainer
        from transformers import AutoConfig, AutoModelForCausalLM

        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¨¡å‹
        config = AutoConfig.from_pretrained("gpt2")
        config.vocab_size = 1000  # å‡å°è¯æ±‡è¡¨å¤§å°ä»¥èŠ‚çœå†…å­˜
        config.n_layer = 2  # å‡å°‘å±‚æ•°
        config.n_head = 2   # å‡å°‘æ³¨æ„åŠ›å¤´æ•°
        config.n_embd = 128 # å‡å°‘åµŒå…¥ç»´åº¦

        model = AutoModelForCausalLM.from_config(config)

        # åˆ›å»ºåŸºæœ¬çš„è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir="./test_output",
            per_device_train_batch_size=1,
            num_train_epochs=1,
            logging_steps=1,
        )

        # åˆ›å»ºåŠ¨æ€é•¿åº¦é…ç½®
        dynamic_config = {
            'enable_dynamic_length': True,
            'initial_length': 64,
            'max_length': 2048,
            'confidence_threshold': 0.7,
            'max_expansions': 3
        }

        # åˆå§‹åŒ–trainer
        trainer = DynamicLengthTrainer(
            model=model,
            args=training_args,
            dynamic_config=dynamic_config
        )
        
        logger.info("âœ… DynamicLengthTraineråˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"   - enable_dynamic_length: {trainer.enable_dynamic_length}")
        logger.info(f"   - dynamic_config: {trainer.dynamic_config}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ DynamicLengthTraineråˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def test_compute_loss_method():
    """æµ‹è¯•compute_lossæ–¹æ³•æ˜¯å¦å­˜åœ¨ä¸”å¯è°ƒç”¨"""
    try:
        from trainer.dynamic_length_trainer import DynamicLengthTrainer
        from transformers import AutoConfig, AutoModelForCausalLM

        # åˆ›å»ºç®€å•æµ‹è¯•æ¨¡å‹
        config = AutoConfig.from_pretrained("gpt2")
        config.vocab_size = 1000
        config.n_layer = 1
        config.n_head = 2
        config.n_embd = 64
        model = AutoModelForCausalLM.from_config(config)

        training_args = TrainingArguments(
            output_dir="./test_output",
            per_device_train_batch_size=1,
            num_train_epochs=1,
        )

        dynamic_config = {
            'enable_dynamic_length': False,  # å…ˆæµ‹è¯•æ ‡å‡†æ¨¡å¼
        }

        trainer = DynamicLengthTrainer(
            model=model,
            args=training_args,
            dynamic_config=dynamic_config
        )
        
        # æ£€æŸ¥compute_lossæ–¹æ³•æ˜¯å¦å­˜åœ¨
        assert hasattr(trainer, 'compute_loss'), "compute_lossæ–¹æ³•ä¸å­˜åœ¨"
        assert callable(trainer.compute_loss), "compute_lossä¸å¯è°ƒç”¨"
        
        logger.info("âœ… compute_lossæ–¹æ³•æ£€æŸ¥é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ compute_lossæ–¹æ³•æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_dynamic_length_detection():
    """æµ‹è¯•åŠ¨æ€é•¿åº¦æ¨¡å¼æ£€æµ‹é€»è¾‘"""
    try:
        from trainer.dynamic_length_trainer import DynamicLengthTrainer
        from transformers import AutoConfig, AutoModelForCausalLM

        # åˆ›å»ºç®€å•æµ‹è¯•æ¨¡å‹
        config = AutoConfig.from_pretrained("gpt2")
        config.vocab_size = 1000
        config.n_layer = 1
        config.n_head = 2
        config.n_embd = 64
        model = AutoModelForCausalLM.from_config(config)

        training_args = TrainingArguments(
            output_dir="./test_output",
            per_device_train_batch_size=1,
            num_train_epochs=1,
        )

        # æµ‹è¯•å¯ç”¨åŠ¨æ€é•¿åº¦
        dynamic_config = {
            'enable_dynamic_length': True,
        }

        trainer = DynamicLengthTrainer(
            model=model,
            args=training_args,
            dynamic_config=dynamic_config
        )
        
        # æ¨¡æ‹Ÿæ ‡å‡†è¾“å…¥ï¼ˆåº”è¯¥å›é€€åˆ°æ ‡å‡†è®­ç»ƒï¼‰
        standard_inputs = {
            'input_ids': torch.tensor([[1, 2, 3, 4, 5]]),
            'labels': torch.tensor([[1, 2, 3, 4, 5]])
        }
        
        # æ¨¡æ‹ŸåŠ¨æ€é•¿åº¦è¾“å…¥
        dynamic_inputs = {
            'input_ids': torch.tensor([[1, 2, 3, 4, 5]]),
            'labels': torch.tensor([[1, 2, 3, 4, 5]]),
            'prompt_lengths': torch.tensor([2])
        }
        
        logger.info("âœ… åŠ¨æ€é•¿åº¦æ£€æµ‹é€»è¾‘æµ‹è¯•å‡†å¤‡å®Œæˆ")
        logger.info(f"   - æ ‡å‡†è¾“å…¥åŒ…å«prompt_lengths: {'prompt_lengths' in standard_inputs}")
        logger.info(f"   - åŠ¨æ€è¾“å…¥åŒ…å«prompt_lengths: {'prompt_lengths' in dynamic_inputs}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ åŠ¨æ€é•¿åº¦æ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹æµ‹è¯•åŠ¨æ€é•¿åº¦è®­ç»ƒé›†æˆ...")
    
    tests = [
        ("å¯¼å…¥æµ‹è¯•", test_dynamic_length_trainer_import),
        ("åˆå§‹åŒ–æµ‹è¯•", test_dynamic_length_trainer_init),
        ("compute_lossæ–¹æ³•æµ‹è¯•", test_compute_loss_method),
        ("åŠ¨æ€é•¿åº¦æ£€æµ‹æµ‹è¯•", test_dynamic_length_detection),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        logger.info(f"\nğŸ“‹ è¿è¡Œæµ‹è¯•: {test_name}")
        if test_func():
            passed += 1
        else:
            logger.error(f"æµ‹è¯•å¤±è´¥: {test_name}")
    
    logger.info(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åŠ¨æ€é•¿åº¦è®­ç»ƒé›†æˆæˆåŠŸ")
    else:
        logger.error("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤é—®é¢˜")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
