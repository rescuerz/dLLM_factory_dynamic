import torch
import torch.nn.functional as F
from transformers import Trainer
from transformers import DefaultDataCollator
import random
from tqdm import tqdm
import pickle
import torch.distributed as dist
import json

SYSTEM_PROMPT = """
Respond in the following format:
<reasoning>
Your reasoning here
</reasoning>
<answer>
...
</answer>
"""



def insert_special_tokens(tokenizer, inputs: str, prompt_text: str, original_response: str) -> str:
    """
    åœ¨apply_chat_templateåæ’å…¥ç‰¹æ®Štoken

    Args:
        tokenizer: åˆ†è¯å™¨
        inputs: apply_chat_templateåçš„å®Œæ•´æ–‡æœ¬ (prompt + response)
        prompt_text: apply_chat_templateåçš„promptéƒ¨åˆ†
        original_response: åŸå§‹responseæ–‡æœ¬å†…å®¹

    Returns:
        æ’å…¥ç‰¹æ®Štokenåçš„å®Œæ•´æ–‡æœ¬
    """
    # ç‰¹æ®Štokenå®šä¹‰ï¼ˆåœ¨æ•°æ®å¤„ç†ä¸­ç›´æ¥å®šä¹‰ï¼Œé¿å…å¾ªç¯å¯¼å…¥ï¼‰
    SPECIAL_TOKENS = {
        "expand": "<|expand|>",  # æ‰©å±•token
        "enough": "<|enough|>"   # ç»“æŸtoken
    }

    try:
        # 1. è®¡ç®—åŸå§‹responseçš„tokené•¿åº¦ï¼ˆç”¨äºå†³å®šæ’å…¥ç­–ç•¥ï¼‰
        original_response_tokens = tokenizer.encode(original_response, add_special_tokens=False)
        original_response_length = len(original_response_tokens)

        # 2. å¯¹å®Œæ•´æ–‡æœ¬è¿›è¡Œtokenization
        full_tokens = tokenizer.encode(inputs, add_special_tokens=False)
        prompt_tokens = tokenizer.encode(prompt_text, add_special_tokens=False)

        # 3. è®¡ç®—responseéƒ¨åˆ†åœ¨å®Œæ•´tokenåºåˆ—ä¸­çš„èµ·å§‹ä½ç½®
        response_start_idx = len(prompt_tokens)

        # 4. è·å–ç‰¹æ®Štokençš„IDï¼Œå¹¶æ£€æŸ¥æ˜¯å¦æœ‰æ•ˆ
        enough_token_id = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS['enough'])
        expand_token_id = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS['expand'])

        if enough_token_id is None or expand_token_id is None:
            print(f"Warning: Special tokens not found in tokenizer vocabulary")
            print(f"è¯·ç¡®ä¿åœ¨è®­ç»ƒè„šæœ¬ä¸­è°ƒç”¨äº† setup_model_and_tokenizer_for_special_tokens å‡½æ•°")
            return inputs

        # 5. æ ¹æ®åŸå§‹responseé•¿åº¦å†³å®šæ’å…¥ç­–ç•¥
        if original_response_length <= 64:
            # çŸ­å›ç­”ï¼šåœ¨æœ«å°¾æ·»åŠ <|enough|>token
            modified_tokens = full_tokens.copy()
            modified_tokens.append(enough_token_id)
            # print(f"Inserted <|enough|> at the end for short response (length: {original_response_length})")
        else:
            # é•¿å›ç­”ï¼šåœ¨æŒ‡å®šä½ç½®æ’å…¥<|expand|>token
            target_positions = [63, 127, 255, 511, 1023]  # ç¬¬64ã€128ã€256ã€512ã€1024ä¸ªtoken

            # å…ˆæå–responseéƒ¨åˆ†çš„token
            response_tokens = full_tokens[response_start_idx:].copy()

            # ä»å‰å¾€åæ’å…¥ï¼Œç¡®ä¿æ¯ä¸ª<|expand|> tokenæœ€ç»ˆä½äºæ­£ç¡®ä½ç½®
            for final_pos in target_positions:
                # æ£€æŸ¥åŸå§‹responseé•¿åº¦æ˜¯å¦è¶³å¤Ÿé•¿ï¼Œéœ€è¦æ’å…¥è¿™ä¸ªä½ç½®çš„token
                if final_pos < original_response_length:
                    # ç›´æ¥åœ¨ç›®æ ‡æœ€ç»ˆä½ç½®æ’å…¥
                    # ç”±äºæˆ‘ä»¬ä»å‰å¾€åæ’å…¥ï¼Œå½“å‰çš„final_poså°±æ˜¯æˆ‘ä»¬è¦æ’å…¥çš„ä½ç½®
                    insert_pos = final_pos

                    # ç¡®ä¿æ’å…¥ä½ç½®åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    if insert_pos >= 0 and insert_pos < len(response_tokens):
                        # åœ¨è®¡ç®—å‡ºçš„ä½ç½®æ’å…¥expand token ID
                        response_tokens.insert(insert_pos, expand_token_id)
                        # print(f"Inserted <|expand|> at response position {insert_pos}, final position will be {final_pos}")

            # é‡æ–°ç»„åˆå®Œæ•´çš„tokenåºåˆ—
            modified_tokens = full_tokens[:response_start_idx] + response_tokens

        # 6. è§£ç ä¸ºæ–‡æœ¬
        modified_inputs = tokenizer.decode(modified_tokens, skip_special_tokens=False)
        return modified_inputs

    except Exception as e:
        print(f"Error inserting special tokens: {e}")
        return inputs  # å‡ºé”™æ—¶è¿”å›åŸå§‹æ–‡æœ¬

def preprocess_dataset_original(data, tokenizer, max_length, test_split=0.01):
    preprocessed_data = []
    for i in tqdm(range(len(data)), desc="Preprocessing dataset"):
        question = SYSTEM_PROMPT + "\n\n" + data[i]["question"]
        trajectory = f"<reasoning>{data[i]['thinking_trajectories'][0]}</reasoning>\n<answer>{data[i]['attempt']}</answer>"
        prompt = [{"role": "user", "content": question}]
        response = [{"role": "assistant", "content": trajectory}]
        inputs = tokenizer.apply_chat_template(prompt + response, tokenize=False)
        prompt = tokenizer.apply_chat_template(prompt, tokenize=False) + "\n"
        tokenized_input = tokenizer(
            inputs, return_tensors="pt", truncation=True, max_length=max_length, padding="max_length"
        ).input_ids.squeeze(0)
        num_tokens = tokenized_input.shape[0]
        tokenized_prompt = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=max_length)
        preprocessed_data.append(
            {
                "input_ids": tokenized_input,
                "prompt_lengths": tokenized_prompt.attention_mask.sum(-1),
            }
        )

    random.shuffle(preprocessed_data)
    test_data = preprocessed_data[: int(len(preprocessed_data) * test_split)]
    train_data = preprocessed_data[int(len(preprocessed_data) * test_split) :]
    return train_data, test_data

def preprocess_dataset(data, tokenizer, max_length, test_split=0.01):
    """
    æ•°æ®æ ¼å¼ï¼š
    {
        'question': 'Solve: 2x + 3 = 7',
        'thinking_trajectories': ['First, subtract 3 from both sides: 2x = 4. Then divide by 2: x = 2.'],
        'attempt': 'x = 2'
    }
    # æ­¥éª¤1ï¼šæ„å»ºquestion
    question = 
    Respond in the following format:
    <reasoning>
    Your reasoning here
    </reasoning>
    <answer>
    ...
    </answer>

    Solve: 2x + 3 = 7
    # æ­¥éª¤2ï¼šæ„å»ºtrajectory
    trajectory = "<reasoning>First, subtract 3 from both sides: 2x = 4. Then divide by 2: x = 2.</reasoning>\n<answer>x = 2</answer>"

    # æ­¥éª¤3-4ï¼šåº”ç”¨èŠå¤©æ¨¡æ¿
    inputs = "<|im_start|>user\n[questionå†…å®¹]\n<|im_end|>\n<|im_start|>assistant\n[trajectoryå†…å®¹]\n<|im_end|>"

    # æ­¥éª¤5ï¼šåˆ†è¯
    tokenized_input = tensor([1, 123, 456, 789, ..., 0, 0, 0])  # [4096]

    # æ­¥éª¤6ï¼šè¾“å‡º
    output_sample = {
        "input_ids": tensor([1, 123, 456, 789, ..., 0, 0, 0]),
        "prompt_lengths": tensor(156)  # ç”¨æˆ·è¾“å…¥éƒ¨åˆ†é•¿åº¦
    }
    """
    preprocessed_data = []

    # æ¸…ç©ºexample.jsonlæ–‡ä»¶ï¼Œé¿å…é‡å¤æ•°æ®
    with open('example.jsonl', 'w', encoding='utf-8') as f:
        pass  # åˆ›å»ºç©ºæ–‡ä»¶
    with open('example_processed.jsonl', 'w', encoding='utf-8') as f:
        pass  # åˆ›å»ºç©ºæ–‡ä»¶
    print("å·²æ¸…ç©ºexample.jsonlå’Œexample_processed.jsonlæ–‡ä»¶ï¼Œå‡†å¤‡ä¿å­˜æ–°çš„å¤„ç†æ•°æ®")

    for i in tqdm(range(len(data)), desc="Preprocessing dataset"):
        # æ„å»ºç³»ç»Ÿæç¤ºè¯ + é—®é¢˜
        question = SYSTEM_PROMPT + "\n\n" + data[i]["question"]
        # æ ¹æ®æ•°æ®é›†æ ¼å¼æ„å»ºæ¨ç†è½¨è¿¹ + ç­”æ¡ˆ
        if 'thinking_trajectories' in data[i] and 'attempt' in data[i]:
            # simplescaling/s1K æ ¼å¼
            # print("simplescaling/s1K æ ¼å¼")
            reasoning = data[i]['thinking_trajectories'][0]
            answer = data[i]['attempt']
        elif 'answer' in data[i]:
            # GSM8K ç­‰æ ‡å‡†æ ¼å¼ï¼Œä»answerä¸­æå–æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
            full_answer = data[i]['answer']
            # å°è¯•åˆ†ç¦»æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
            # å¦‚æœanswerä¸­åŒ…å« #### å¼€å¤´ï¼Œåˆ™è®¤ä¸º #### åé¢çš„å†…å®¹æ˜¯æœ€ç»ˆç­”æ¡ˆ
            if '####' in full_answer:
                # print("GSM8K æ ¼å¼")
                answer = full_answer.split('####')[1].strip()
                reasoning = full_answer.split('####')[0].strip()
            else:
                if '\n' in full_answer:
                    parts = full_answer.split('\n')
                    reasoning = '\n'.join(parts[:-1]).strip()
                    answer = parts[-1].strip()
                else:
                    reasoning = full_answer
                    answer = full_answer
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼ï¼Œæ ·æœ¬ {i} ç¼ºå°‘å¿…è¦å­—æ®µ")

        trajectory = f"<reasoning>{reasoning}</reasoning>\n<answer>{answer}</answer>"
        # æ„å»ºå¯¹è¯æ ¼å¼
        prompt = [{"role": "user", "content": question}]
        response = [{"role": "assistant", "content": trajectory}]
        
        sample = {
            "sample_id": i,
            "prompt": prompt,
            "response": response,
        }
        with open('example.jsonl', 'a', encoding='utf-8') as f:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')

        # apply_chat_template ä½œç”¨æœºåˆ¶ï¼šå°†å¯¹è¯æ ¼å¼è½¬æ¢ä¸ºæ¨¡å‹ç‰¹å®šçš„èŠå¤©æ ¼å¼
        # æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚ <|im_start|>, <|im_end|> ç­‰ï¼‰
        inputs = tokenizer.apply_chat_template(prompt + response, tokenize=False)
        prompt = tokenizer.apply_chat_template(prompt, tokenize=False) + "\n"

        # æ’å…¥ç‰¹æ®Štokenï¼š<|expand|>åœ¨é•¿å›ç­”çš„æŒ‡å®šä½ç½®ï¼Œ<|enough|>åœ¨çŸ­å›ç­”æœ«å°¾
        inputs = insert_special_tokens(tokenizer, inputs, prompt, trajectory)

        # å°†å¤„ç†å¥½çš„æ•°æ®ä¿å­˜åˆ°example.jsonlæ–‡ä»¶ä¸­
        processed_sample = {
            "sample_id": i,
            "processed_inputs": inputs,
        }

        # ä¿å­˜åˆ°example.jsonlæ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        with open('example_processed.jsonl', 'a', encoding='utf-8') as f:
            f.write(json.dumps(processed_sample, ensure_ascii=False) + '\n')
        tokenized_input = tokenizer(
            inputs, return_tensors="pt", truncation=True, max_length=max_length, padding="max_length"
        ).input_ids.squeeze(0)
        num_tokens = tokenized_input.shape[0]
        tokenized_prompt = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=max_length)
        # å•ç‹¬çš„promptç”¨äºè®¡ç®—ç”¨æˆ·è¾“å…¥éƒ¨åˆ†çš„é•¿åº¦ï¼Œåœ¨è®­ç»ƒæ—¶å±è”½è¿™éƒ¨åˆ†çš„æŸå¤±
        preprocessed_data.append(
            {
                "input_ids": tokenized_input,
                "prompt_lengths": tokenized_prompt.attention_mask.sum(-1),
            }
        )

    random.shuffle(preprocessed_data)
    test_data = preprocessed_data[: int(len(preprocessed_data) * test_split)]
    train_data = preprocessed_data[int(len(preprocessed_data) * test_split) :]

    # æ‰“å°ä¿å­˜ä¿¡æ¯
    print(f"âœ… å·²å°† {len(data)} ä¸ªå¤„ç†åçš„æ ·æœ¬ä¿å­˜åˆ° example.jsonl æ–‡ä»¶")
    print(f"ğŸ“Š æ•°æ®åˆ†å‰²: è®­ç»ƒé›† {len(train_data)} æ ·æœ¬, éªŒè¯é›† {len(test_data)} æ ·æœ¬")

    return train_data, test_data






