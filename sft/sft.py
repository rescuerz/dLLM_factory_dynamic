
import torch
import argparse
from transformers import TrainingArguments
import os
from data import dLLMSFTDataset,dLLMDataCollator,preprocess_dataset
from trainer import dLLMTrainer
from argsparser import ArgsProcessor
from utils import TransformerModelLoader,LoraBuilder
from datasets import load_dataset

# Special Tokenå®šä¹‰
SPECIAL_TOKENS = {
    "expand": "<|expand|>",  # æ‰©å±•token
    "enough": "<|enough|>"   # ç»“æŸtoken
}

def ensure_special_tokens_in_tokenizer(tokenizer):
    """
    ç¡®ä¿ç‰¹æ®Štokenåœ¨tokenizerè¯æ±‡è¡¨ä¸­

    Returns:
        bool: æ˜¯å¦æ·»åŠ äº†æ–°çš„ç‰¹æ®Štoken
    """
    special_tokens = list(SPECIAL_TOKENS.values())
    existing_tokens = set(tokenizer.get_vocab().keys())
    new_tokens = [token for token in special_tokens if token not in existing_tokens]

    if new_tokens:
        special_tokens_dict = {'additional_special_tokens': new_tokens}
        num_added = tokenizer.add_special_tokens(special_tokens_dict)
        print(f"è®­ç»ƒåˆå§‹åŒ–ï¼šæ·»åŠ äº† {num_added} ä¸ªç‰¹æ®Štoken: {new_tokens}")
        return True
    return False

def setup_model_and_tokenizer_for_special_tokens(model, tokenizer):
    """
    ä¸ºè®­ç»ƒè„šæœ¬æä¾›çš„å·¥å…·å‡½æ•°ï¼šè®¾ç½®æ¨¡å‹å’Œtokenizerä»¥æ”¯æŒç‰¹æ®Štoken

    Args:
        model: é¢„è®­ç»ƒæ¨¡å‹
        tokenizer: é¢„è®­ç»ƒtokenizer

    Returns:
        tuple: (model, tokenizer, tokens_added) - æ˜¯å¦æ·»åŠ äº†æ–°token
    """
    # è®°å½•åŸå§‹çŠ¶æ€
    original_tokenizer_size = len(tokenizer)
    original_model_embedding_size = model.get_input_embeddings().weight.size(0)

    print(f"åŸå§‹çŠ¶æ€æ£€æŸ¥:")
    print(f"  Tokenizerè¯æ±‡è¡¨å¤§å°: {original_tokenizer_size}")
    print(f"  æ¨¡å‹embeddingå±‚å¤§å°: {original_model_embedding_size}")

    # æ£€æŸ¥åŸå§‹çŠ¶æ€æ˜¯å¦æ­£å¸¸
    if original_model_embedding_size != original_tokenizer_size:
        size_diff = original_model_embedding_size - original_tokenizer_size
        print(f"âš ï¸  è­¦å‘Š: æ¨¡å‹embeddingå±‚ä¸tokenizerå¤§å°ä¸åŒ¹é… (å·®å¼‚: {size_diff})")
        if size_diff < 0:
            print(f"âŒ ä¸¥é‡é”™è¯¯: æ¨¡å‹embeddingå±‚å°äºtokenizerè¯æ±‡è¡¨ï¼Œè¿™ä¼šå¯¼è‡´è®­ç»ƒé”™è¯¯")
            raise ValueError(f"æ¨¡å‹embeddingå±‚({original_model_embedding_size}) < tokenizerè¯æ±‡è¡¨({original_tokenizer_size})")

    tokens_added = ensure_special_tokens_in_tokenizer(tokenizer)

    if tokens_added:
        new_tokenizer_size = len(tokenizer)
        expected_new_embedding_size = max(original_model_embedding_size, new_tokenizer_size)

        print(f"ç‰¹æ®Štokenæ·»åŠ å:")
        print(f"  æ–°tokenizerè¯æ±‡è¡¨å¤§å°: {new_tokenizer_size}")
        print(f"  é¢„æœŸæ¨¡å‹embeddingå±‚å¤§å°: {expected_new_embedding_size}")

        # å®‰å…¨çš„embeddingå±‚è°ƒæ•´
        if new_tokenizer_size > original_model_embedding_size:
            # åªæœ‰å½“tokenizerå˜å¤§æ—¶æ‰è°ƒæ•´æ¨¡å‹
            print(f"æ­£åœ¨æ‰©å±•æ¨¡å‹embeddingå±‚: {original_model_embedding_size} -> {new_tokenizer_size}")
            model.resize_token_embeddings(new_tokenizer_size)
            actual_new_size = model.get_input_embeddings().weight.size(0)
            print(f"âœ… æ¨¡å‹embeddingå±‚å·²æ‰©å±•: {original_model_embedding_size} -> {actual_new_size}")
        elif new_tokenizer_size == original_model_embedding_size:
            print(f"âœ… æ¨¡å‹embeddingå±‚å¤§å°å·²åŒ¹é…ï¼Œæ— éœ€è°ƒæ•´")

        # éªŒè¯æœ€ç»ˆçŠ¶æ€
        final_tokenizer_size = len(tokenizer)
        final_model_embedding_size = model.get_input_embeddings().weight.size(0)

        if final_model_embedding_size >= final_tokenizer_size:
            print(f"âœ… æœ€ç»ˆçŠ¶æ€éªŒè¯é€šè¿‡:")
            print(f"  Tokenizer: {final_tokenizer_size}, æ¨¡å‹embedding: {final_model_embedding_size}")
        else:
            print(f"âŒ æœ€ç»ˆçŠ¶æ€éªŒè¯å¤±è´¥:")
            print(f"  Tokenizer: {final_tokenizer_size}, æ¨¡å‹embedding: {final_model_embedding_size}")
            raise ValueError("æ¨¡å‹embeddingå±‚å°äºtokenizerè¯æ±‡è¡¨ï¼Œè¿™ä¼šå¯¼è‡´è®­ç»ƒé”™è¯¯")

        # è®¾ç½®pad tokenï¼ˆå¦‚æœéœ€è¦ï¼‰
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f"è®¾ç½®pad_tokenä¸º: {tokenizer.pad_token}")
    else:
        print(f"â„¹ï¸  ç‰¹æ®Štokenå·²å­˜åœ¨ï¼Œæ— éœ€è°ƒæ•´æ¨¡å‹embeddingå±‚")

    return model, tokenizer, tokens_added
def load_data(args, tokenizer):
    # å¦‚æœæ˜¯æœ¬åœ°jsonæ–‡ä»¶ï¼Œåˆ™ç›´æ¥åŠ è½½
    if args.train_data.endswith('.json'):
        from datasets import Dataset
        import json
        with open(args.train_data, 'r', encoding='utf-8') as f:
            data = json.load(f)
        data = Dataset.from_list(data)
    # å¦‚æœæ˜¯HuggingFaceæ•°æ®é›†ï¼Œåˆ™ä½¿ç”¨load_datasetä»huggingfaceä¸‹è½½å¹¶åŠ è½½æ•°æ®é›†
    else:
        print("ä»HuggingFace HubåŠ è½½æ•°æ®é›†...")

        # å¤„ç†ç‰¹æ®Šæ•°æ®é›†çš„é…ç½®
        dataset_config = None
        if args.train_data == "gsm8k":
            dataset_config = "main"  # gsm8k é»˜è®¤ä½¿ç”¨ main é…ç½®
            print(f"æ£€æµ‹åˆ° gsm8k æ•°æ®é›†ï¼Œä½¿ç”¨é…ç½®: {dataset_config}")

        # åŠ è½½æ•°æ®é›†
        if dataset_config:
            data = load_dataset(args.train_data, dataset_config, split="train")
        else:
            data = load_dataset(args.train_data, split="train")

        data_len = len(data)  # type: ignore
        print(f"æˆåŠŸä» {args.train_data} åŠ è½½äº† {data_len} ä¸ªè®­ç»ƒæ ·æœ¬")

    # å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†
    train_data, eval_data = preprocess_dataset(data, tokenizer, args.max_length)
    print("Train data length: ", len(train_data))
    print("Eval data length: ", len(eval_data))
    train_dataset = dLLMSFTDataset(train_data, tokenizer, args.max_length)
    eval_dataset = dLLMSFTDataset(eval_data, tokenizer, args.max_length, eval=True)
    return train_dataset, eval_dataset

def train_model(args, model, tokenizer, train_dataset, eval_dataset):
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨åŠ¨æ€é•¿åº¦å¾®è°ƒ
    enable_dynamic_length = getattr(args, 'enable_dynamic_length', False)
    dynamic_config = getattr(args, 'dynamic_length', None) if enable_dynamic_length else None

    # å°†enable_dynamic_lengthæ·»åŠ åˆ°dynamic_configä¸­
    if enable_dynamic_length and dynamic_config:
        dynamic_config['enable_dynamic_length'] = enable_dynamic_length

    print(f"ğŸ”§ è®­ç»ƒæ¨¡å¼: {'åŠ¨æ€é•¿åº¦å¾®è°ƒ' if enable_dynamic_length else 'æ ‡å‡†SFTè®­ç»ƒ'}")
    if enable_dynamic_length:
        print(f"ğŸ“Š åŠ¨æ€é•¿åº¦é…ç½®: {dynamic_config}")

    # åˆ›å»ºè®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=os.path.join(args.output_dir, args.job_name),
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.local_batch_size,
        gradient_accumulation_steps=args.grad_accum_steps,
        evaluation_strategy=args.evaluation_strategy,
        eval_steps=args.eval_steps,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        learning_rate=args.learning_rate,
        load_best_model_at_end=args.load_best_model_at_end,
        weight_decay=args.weight_decay,
        max_grad_norm=args.max_grad_norm,
        bf16=args.bf16,
        report_to=args.report_to,
        remove_unused_columns=args.remove_unused_columns,
    )

    # æ ¹æ®é…ç½®é€‰æ‹©è®­ç»ƒå™¨å’Œæ•°æ®æ•´ç†å™¨
    if enable_dynamic_length:
        # ä½¿ç”¨åŠ¨æ€é•¿åº¦è®­ç»ƒå™¨
        from trainer.dynamic_length_trainer import DynamicLengthTrainer

        # åˆ›å»ºæ”¯æŒåŠ¨æ€é•¿åº¦çš„æ•°æ®æ•´ç†å™¨
        data_collator = dLLMDataCollator(
            tokenizer=tokenizer,
            mask_token_id=126336,
            max_length=args.max_length,
            enable_dynamic_length=True,
            dynamic_config=dynamic_config
        )

        # åˆ›å»ºåŠ¨æ€é•¿åº¦è®­ç»ƒå™¨
        trainer = DynamicLengthTrainer(
            model=model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            dynamic_config=dynamic_config,
            tokenizer=tokenizer  # ä¼ é€’tokenizerç»™è®­ç»ƒå™¨
        )

        print("âœ… åŠ¨æ€é•¿åº¦è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")

    else:
        # ä½¿ç”¨æ ‡å‡†è®­ç»ƒå™¨ï¼ˆä¿æŒç°æœ‰é€»è¾‘ä¸å˜ï¼‰
        data_collator = dLLMDataCollator(
            tokenizer=tokenizer,
            mask_token_id=126336,
            max_length=args.max_length
        )

        trainer = dLLMTrainer(
            model=model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
        )

        print("âœ… æ ‡å‡†dLLMè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")

    # å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Configuration parser")
    parser.add_argument("--debug",dest="debug",action="store_true",help="debug mode")
    parser.add_argument("--enable_lora",default=True,help="enable lora")
    parser.add_argument("--train_config_path",type=str,default="./config/sft/default_config.yaml",help="Path to the Train YAML configuration file")
    parser.add_argument("--lora_config_path",type=str,default="./config/lora/default_config.yaml",help="Path to the Lora YAML configuration file")
    args = parser.parse_args()
    args_processor = ArgsProcessor(args.train_config_path)
    args = args_processor.add_args_from_yaml(args)
    model_loader = TransformerModelLoader(tokenizer_path=args.model_name,model_path=args.model_name)
    tokenizer, model = model_loader.load_model_tokenizer()

    # è®¾ç½®ç‰¹æ®Štokenå¹¶è°ƒæ•´æ¨¡å‹embeddingå±‚
    model, tokenizer, tokens_added = setup_model_and_tokenizer_for_special_tokens(model, tokenizer)
    if tokens_added:
        print("âœ… ç‰¹æ®Štokenè®¾ç½®å®Œæˆï¼Œæ¨¡å‹å·²å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒ")

    if args.enable_lora:
        lora_args =  argparse.ArgumentParser(description="Lora Configuration parser").parse_args()
        lora_args_processor = ArgsProcessor(args.lora_config_path)
        lora_args = lora_args_processor.add_args_from_yaml(lora_args)
        lora_bulider = LoraBuilder(lora_args)
        model = lora_bulider.get_Lora(model)
    train_dataset, eval_dataset = load_data(args, tokenizer)
    print("Global Batch Size",args.local_batch_size * args.grad_accum_steps * torch.cuda.device_count())
    train_model(args,model,tokenizer,train_dataset,eval_dataset)
    