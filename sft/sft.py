
import torch
import argparse
from transformers import TrainingArguments
import os
from data import dLLMSFTDataset,dLLMDataCollator,dLLMDataCollator_dynamic_length,preprocess_dataset
from trainer import dLLMTrainer
from argsparser import ArgsProcessor
from utils import TransformerModelLoader,LoraBuilder
from datasets import load_dataset
import logging

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
def setup_logging(debug=False):
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    level = logging.DEBUG if debug else logging.INFO

    # é…ç½®æ ¹æ—¥å¿—å™¨
    logging.basicConfig(
        level=level,
        format='%(message)s',
        handlers=[
            logging.StreamHandler()
        ]
    )

    # è®¾ç½®ç‰¹å®šæ¨¡å—çš„æ—¥å¿—çº§åˆ«
    logging.getLogger('sft.trainer.dynamic_length_trainer').setLevel(level)
    logging.getLogger('transformers').setLevel(logging.WARNING)  # å‡å°‘transformersçš„æ—¥å¿—å™ªéŸ³

    if debug:
        print("ğŸ› DEBUGæ¨¡å¼å·²å¯ç”¨ - å°†æ˜¾ç¤ºè¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯")

    return logging.getLogger(__name__)

logger = logging.getLogger(__name__)

# Special Tokenå®šä¹‰
SPECIAL_TOKENS = {
    "expand": "<|expand|>",  # æ‰©å±•token
    "enough": "<|enough|>"   # ç»“æŸtoken
}

def ensure_special_tokens_in_tokenizer(tokenizer):
    """
    ç¡®ä¿ç‰¹æ®Štokenåœ¨tokenizerè¯æ±‡è¡¨ä¸­

    Returns:
        bool: æ˜¯å¦æ·»åŠ äº†æ–°çš„ç‰¹æ®Štoken
    """
    special_tokens = list(SPECIAL_TOKENS.values())
    existing_tokens = set(tokenizer.get_vocab().keys())
    new_tokens = [token for token in special_tokens if token not in existing_tokens]

    if new_tokens:
        special_tokens_dict = {'additional_special_tokens': new_tokens}
        num_added = tokenizer.add_special_tokens(special_tokens_dict)
        logger.info(f"è®­ç»ƒåˆå§‹åŒ–ï¼šæ·»åŠ äº† {num_added} ä¸ªç‰¹æ®Štoken: {new_tokens}")
        return True
    return False

def setup_model_and_tokenizer_for_special_tokens(model, tokenizer):
    """
    ä¸ºè®­ç»ƒè„šæœ¬æä¾›çš„å·¥å…·å‡½æ•°ï¼šè®¾ç½®æ¨¡å‹å’Œtokenizerä»¥æ”¯æŒç‰¹æ®Štoken

    Args:
        model: é¢„è®­ç»ƒæ¨¡å‹
        tokenizer: é¢„è®­ç»ƒtokenizer

    Returns:
        tuple: (model, tokenizer, tokens_added) - æ˜¯å¦æ·»åŠ äº†æ–°token
    """
    # è®°å½•åŸå§‹çŠ¶æ€
    original_tokenizer_size = len(tokenizer)
    original_model_embedding_size = model.get_input_embeddings().weight.size(0)
    logger.info(f"åŸå§‹çŠ¶æ€ Tokenizerè¯æ±‡è¡¨å¤§å°: {original_tokenizer_size} æ¨¡å‹embeddingå±‚å¤§å°: {original_model_embedding_size}")

    # æ£€æŸ¥åŸå§‹çŠ¶æ€æ˜¯å¦æ­£å¸¸
    if original_model_embedding_size != original_tokenizer_size:
        size_diff = original_model_embedding_size - original_tokenizer_size
        logger.warning(f"âš ï¸  è­¦å‘Š: æ¨¡å‹embeddingå±‚ä¸tokenizerå¤§å°ä¸åŒ¹é… (å·®å¼‚: {size_diff})")
        if size_diff < 0:
            logger.error(f"âŒ ä¸¥é‡é”™è¯¯: æ¨¡å‹embeddingå±‚å°äºtokenizerè¯æ±‡è¡¨ï¼Œè¿™ä¼šå¯¼è‡´è®­ç»ƒé”™è¯¯")
            raise ValueError(f"æ¨¡å‹embeddingå±‚({original_model_embedding_size}) < tokenizerè¯æ±‡è¡¨({original_tokenizer_size})")

    # æ·»åŠ ç‰¹æ®Štoken
    special_tokens = list(SPECIAL_TOKENS.values())
    existing_tokens = set(tokenizer.get_vocab().keys())
    new_tokens = [token for token in special_tokens if token not in existing_tokens]
    if new_tokens:
        tokens_added = True
        special_tokens_dict = {'additional_special_tokens': new_tokens}
        num_added = tokenizer.add_special_tokens(special_tokens_dict)
        logger.info(f"è®­ç»ƒåˆå§‹åŒ–ï¼šæ·»åŠ äº† {num_added} ä¸ªç‰¹æ®Štoken: {new_tokens}")

        new_tokenizer_size = len(tokenizer)
        actual_new_embedding_size = max(original_model_embedding_size, new_tokenizer_size)

        # å®‰å…¨çš„embeddingå±‚è°ƒæ•´
        if new_tokenizer_size > original_model_embedding_size:
            # åªæœ‰å½“tokenizerå˜å¤§æ—¶æ‰è°ƒæ•´æ¨¡å‹
            model.resize_token_embeddings(new_tokenizer_size)
            actual_new_embedding_size = model.get_input_embeddings().weight.size(0)
            logger.info(f"æ¨¡å‹embeddingå±‚å·²æ‰©å±•: {original_model_embedding_size} -> {actual_new_embedding_size}")
        elif new_tokenizer_size == original_model_embedding_size:
            logger.info(f"æ¨¡å‹embeddingå±‚å¤§å°å·²åŒ¹é…ï¼Œæ— éœ€è°ƒæ•´")

        logger.info(f"æ’å…¥ç‰¹æ®Štokenåï¼ŒTokenizerè¯æ±‡è¡¨å¤§å°: {new_tokenizer_size} æ¨¡å‹embeddingå±‚å¤§å°: {actual_new_embedding_size}")

    else:
        tokens_added = False
        logger.info(f"ç‰¹æ®Štokenå·²å­˜åœ¨ï¼Œæ— éœ€è°ƒæ•´æ¨¡å‹embeddingå±‚")

    return model, tokenizer, tokens_added

def load_data(args, tokenizer):
    # å¦‚æœæ˜¯æœ¬åœ°jsonæ–‡ä»¶ï¼Œåˆ™ç›´æ¥åŠ è½½
    if args.train_data.endswith('.json'):
        from datasets import Dataset
        import json
        with open(args.train_data, 'r', encoding='utf-8') as f:
            data = json.load(f)
        data = Dataset.from_list(data)
    # å¦‚æœæ˜¯HuggingFaceæ•°æ®é›†ï¼Œåˆ™ä½¿ç”¨load_datasetä»huggingfaceä¸‹è½½å¹¶åŠ è½½æ•°æ®é›†
    else:
        # å¤„ç†ç‰¹æ®Šæ•°æ®é›†çš„é…ç½®
        dataset_config = None
        if args.train_data == "gsm8k":
            dataset_config = "main"  # gsm8k é»˜è®¤ä½¿ç”¨ main é…ç½®

        # åŠ è½½æ•°æ®é›†
        if dataset_config:
            data = load_dataset(args.train_data, dataset_config, split="train")
        else:
            data = load_dataset(args.train_data, split="train")

    # å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†
    train_data, eval_data = preprocess_dataset(data, tokenizer, args.max_length)
    logger.info(f"Train data length: {len(train_data)}, Eval data length: {len(eval_data)}")
    train_dataset = dLLMSFTDataset(train_data, tokenizer, args.max_length)
    eval_dataset = dLLMSFTDataset(eval_data, tokenizer, args.max_length, eval=True)
    return train_dataset, eval_dataset

def train_model(args, model, tokenizer, train_dataset, eval_dataset):
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨åŠ¨æ€é•¿åº¦å¾®è°ƒ
    enable_dynamic_length = getattr(args, 'enable_dynamic_length', False)
    dynamic_config = getattr(args, 'dynamic_length', None) if enable_dynamic_length else None

    # å°†enable_dynamic_lengthæ·»åŠ åˆ°dynamic_configä¸­
    if enable_dynamic_length and dynamic_config:
        dynamic_config['enable_dynamic_length'] = enable_dynamic_length

    logger.info(f"ğŸ”§ è®­ç»ƒæ¨¡å¼: {'åŠ¨æ€é•¿åº¦å¾®è°ƒï¼ˆenable_dynamic_length=Trueï¼‰' if enable_dynamic_length else 'æ ‡å‡†SFTè®­ç»ƒ'}")
    if enable_dynamic_length:
        logger.info(f"åŠ¨æ€é•¿åº¦é…ç½®: {dynamic_config}")

    # åˆ›å»ºè®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=os.path.join(args.output_dir, args.job_name),
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.local_batch_size,
        gradient_accumulation_steps=args.grad_accum_steps,
        evaluation_strategy=args.evaluation_strategy,
        eval_steps=args.eval_steps,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        learning_rate=args.learning_rate,
        load_best_model_at_end=args.load_best_model_at_end,
        weight_decay=args.weight_decay,
        max_grad_norm=args.max_grad_norm,
        bf16=args.bf16,
        report_to=args.report_to,
        remove_unused_columns=args.remove_unused_columns,
    )

    # æ ¹æ®é…ç½®é€‰æ‹©è®­ç»ƒå™¨å’Œæ•°æ®æ•´ç†å™¨
    if enable_dynamic_length:
        # ä½¿ç”¨åŠ¨æ€é•¿åº¦è®­ç»ƒå™¨
        from trainer.dynamic_length_trainer import DynamicLengthTrainer

        # åˆ›å»ºåŠ¨æ€é•¿åº¦ä¸“ç”¨çš„æ•°æ®æ•´ç†å™¨ï¼ˆæä¾›å¹²å‡€æ•°æ®ï¼‰
        data_collator = dLLMDataCollator_dynamic_length(
            tokenizer=tokenizer,
            mask_token_id=126336,
            max_length=args.max_length
        )

        # åˆ›å»ºåŠ¨æ€é•¿åº¦è®­ç»ƒå™¨
        trainer = DynamicLengthTrainer(
            model=model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            dynamic_config=dynamic_config,
            processing_class=tokenizer  
        )

    else:
        # ä½¿ç”¨æ ‡å‡†è®­ç»ƒå™¨ï¼ˆä¿æŒç°æœ‰é€»è¾‘ä¸å˜ï¼‰
        data_collator = dLLMDataCollator(
            tokenizer=tokenizer,
            mask_token_id=126336,
            max_length=args.max_length
        )

        trainer = dLLMTrainer(
            model=model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
        )

    # å¼€å§‹è®­ç»ƒ
    logger.info("å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    logger.info("è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Configuration parser")
    parser.add_argument("--debug",dest="debug",action="store_true",help="debug mode")
    parser.add_argument("--enable_lora",default=True,help="enable lora")
    parser.add_argument("--train_config_path",type=str,default="./config/sft/default_config.yaml",help="Path to the Train YAML configuration file")
    parser.add_argument("--lora_config_path",type=str,default="./config/lora/default_config.yaml",help="Path to the Lora YAML configuration file")
    args = parser.parse_args()
    args_processor = ArgsProcessor(args.train_config_path)
    args = args_processor.add_args_from_yaml(args)

    logger = setup_logging(debug=False)
    model_loader = TransformerModelLoader(tokenizer_path=args.model_name,model_path=args.model_name)
    tokenizer, model = model_loader.load_model_tokenizer()

    # è®¾ç½®ç‰¹æ®Štokenå¹¶è°ƒæ•´æ¨¡å‹embeddingå±‚
    model, tokenizer, tokens_added = setup_model_and_tokenizer_for_special_tokens(model, tokenizer)

    if args.enable_lora:
        lora_args =  argparse.ArgumentParser(description="Lora Configuration parser").parse_args()
        lora_args_processor = ArgsProcessor(args.lora_config_path)
        lora_args = lora_args_processor.add_args_from_yaml(lora_args)
        lora_bulider = LoraBuilder(lora_args)
        model = lora_bulider.get_Lora(model)
    train_dataset, eval_dataset = load_data(args, tokenizer)
    logger.info(f"Global Batch Size: {args.local_batch_size * args.grad_accum_steps * torch.cuda.device_count()}")
    train_model(args,model,tokenizer,train_dataset,eval_dataset)
    