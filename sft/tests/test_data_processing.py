"""
å®Œæ•´è®­ç»ƒç¯å¢ƒæ¨¡æ‹Ÿæµ‹è¯•æ¡†æ¶

è¿™ä¸ªæ–‡ä»¶å®Œæ•´åœ°æ¨¡æ‹Ÿå®é™…è®­ç»ƒç¯å¢ƒæ¥éªŒè¯ç‰¹æ®Štokenå¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. ä»sft.pyå¯¼å…¥çœŸå®çš„ç‰¹æ®Štokenå¤„ç†å‡½æ•°
2. åŠ è½½å®Œæ•´çš„æ¨¡å‹å’Œtokenizer
3. ä½¿ç”¨çœŸå®çš„ç‰¹æ®Štokenè®¾ç½®å’Œæ¨¡å‹embeddingè°ƒæ•´æµç¨‹
4. éªŒè¯æ•°æ®é¢„å¤„ç†ä¸­çš„ç‰¹æ®Štokenæ’å…¥
5. æ£€æŸ¥æ•´ä¸ªæµç¨‹çš„æ­£ç¡®æ€§

ä½¿ç”¨æ–¹æ³•ï¼š
    # å®Œæ•´æµ‹è¯•ï¼ˆåŒ…å«è¯¦ç»†ä½ç½®éªŒè¯ï¼‰
    python test_data_processing.py --train_data gsm8k --max_length 1024 --analyze

    # åŸºç¡€æµ‹è¯•
    python test_data_processing.py --train_data gsm8k --max_length 1024

    # æµ‹è¯•å…¶ä»–æ•°æ®é›†
    python test_data_processing.py --train_data simplescaling/s1K --max_length 2048 --analyze
"""

import torch
import argparse
import os
import sys
from datasets import load_dataset

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥SFTæ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data import dLLMSFTDataset, preprocess_dataset
from utils import TransformerModelLoader

# ä»sft.pyå¯¼å…¥çœŸå®çš„ç‰¹æ®Štokenå¤„ç†å‡½æ•°
from sft import SPECIAL_TOKENS, ensure_special_tokens_in_tokenizer, setup_model_and_tokenizer_for_special_tokens



def load_data(args, tokenizer):
    """
    åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†ï¼ˆä½¿ç”¨å·²è®¾ç½®ç‰¹æ®Štokençš„tokenizerï¼‰

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°ï¼ŒåŒ…å«æ•°æ®è·¯å¾„å’Œé…ç½®
        tokenizer: å·²è®¾ç½®ç‰¹æ®Štokençš„åˆ†è¯å™¨å¯¹è±¡

    Returns:
        tuple: (train_dataset, eval_dataset) è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
    """
    print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {args.train_data}")

    # å¦‚æœæ˜¯æœ¬åœ°JSONæ–‡ä»¶ï¼Œåˆ™ç›´æ¥åŠ è½½
    if args.train_data.endswith('.json'):
        from datasets import Dataset
        import json
        print("æ£€æµ‹åˆ°æœ¬åœ°JSONæ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½...")
        with open(args.train_data, 'r', encoding='utf-8') as f:
            data = json.load(f)
        data = Dataset.from_list(data)
        print(f"ä»æœ¬åœ°JSONæ–‡ä»¶åŠ è½½äº† {len(data)} ä¸ªæ ·æœ¬")

    # å¦‚æœæ˜¯HuggingFaceæ•°æ®é›†ï¼ˆå¦‚ simplescaling/s1Kï¼‰ï¼Œåˆ™ä½¿ç”¨load_datasetä»huggingfaceä¸‹è½½å¹¶åŠ è½½æ•°æ®é›†
    else:
        print("ä»HuggingFace HubåŠ è½½æ•°æ®é›†...")

        # å¤„ç†ç‰¹æ®Šæ•°æ®é›†çš„é…ç½®
        dataset_config = None
        if args.train_data == "gsm8k":
            dataset_config = "main"  # gsm8k é»˜è®¤ä½¿ç”¨ main é…ç½®
            print(f"æ£€æµ‹åˆ° gsm8k æ•°æ®é›†ï¼Œä½¿ç”¨é…ç½®: {dataset_config}")

        # åŠ è½½æ•°æ®é›†
        if dataset_config:
            data = load_dataset(args.train_data, dataset_config, split="train")
        else:
            data = load_dataset(args.train_data, split="train")

        data_len = len(data)  # type: ignore
        print(f"æˆåŠŸä» {args.train_data} åŠ è½½äº† {data_len} ä¸ªè®­ç»ƒæ ·æœ¬")

    # æ˜¾ç¤ºæ•°æ®é›†åŸºæœ¬ä¿¡æ¯
    try:
        features = data.features  # type: ignore
        if features is not None:
            print(f"æ•°æ®é›†ç‰¹å¾: {list(features.keys())}")
        else:
            print("æ•°æ®é›†ç‰¹å¾ä¿¡æ¯ä¸ºç©º")
    except AttributeError:
        print("æ— æ³•è·å–æ•°æ®é›†ç‰¹å¾ä¿¡æ¯")

    try:
        first_sample = data[0]  # type: ignore
        if isinstance(first_sample, dict):
            print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é”®: {list(first_sample.keys())}")
        else:
            print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬ç±»å‹: {type(first_sample)}")
    except (IndexError, KeyError, TypeError, AttributeError):
        print("æ— æ³•è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬ä¿¡æ¯")

    # å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†
    print("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    train_data, test_data = preprocess_dataset(data, tokenizer, args.max_length)
    print(f"è®­ç»ƒæ•°æ®é•¿åº¦: {len(train_data)}")
    print(f"éªŒè¯æ•°æ®é•¿åº¦: {len(test_data)}")

    # åˆ›å»ºPyTorchæ•°æ®é›†å¯¹è±¡
    train_dataset = dLLMSFTDataset(train_data, tokenizer, args.max_length)
    test_dataset = dLLMSFTDataset(test_data, tokenizer, args.max_length, eval=True)

    return train_dataset, test_dataset


def verify_special_tokens_setup(model, tokenizer):
    """
    éªŒè¯ç‰¹æ®Štokenè®¾ç½®æ˜¯å¦æ­£ç¡®
    """
    print(f"\n{'='*50}")
    print("éªŒè¯ç‰¹æ®Štokenè®¾ç½®")
    print(f"{'='*50}")

    success = True

    # 1. æ£€æŸ¥ç‰¹æ®Štokenæ˜¯å¦åœ¨tokenizerè¯æ±‡è¡¨ä¸­
    for token_name, token_value in SPECIAL_TOKENS.items():
        token_id = tokenizer.convert_tokens_to_ids(token_value)
        if token_id == tokenizer.unk_token_id or token_id is None:
            print(f"âŒ ç‰¹æ®Štoken {token_value} æœªåœ¨tokenizerè¯æ±‡è¡¨ä¸­æ‰¾åˆ°")
            success = False
        else:
            print(f"âœ… ç‰¹æ®Štoken {token_value} -> ID: {token_id}")

    # 2. æ£€æŸ¥æ¨¡å‹embeddingå±‚å¤§å°æ˜¯å¦ä¸tokenizerè¯æ±‡è¡¨å¤§å°å…¼å®¹
    model_vocab_size = model.get_input_embeddings().weight.size(0)
    tokenizer_vocab_size = len(tokenizer)

    if model_vocab_size >= tokenizer_vocab_size:
        if model_vocab_size == tokenizer_vocab_size:
            print(f"âœ… æ¨¡å‹embeddingå±‚å¤§å°ä¸tokenizerè¯æ±‡è¡¨å¤§å°å®Œå…¨åŒ¹é…: {model_vocab_size}")
        else:
            size_diff = model_vocab_size - tokenizer_vocab_size
            print(f"âœ… æ¨¡å‹embeddingå±‚å¤§å°å…¼å®¹tokenizerè¯æ±‡è¡¨å¤§å°:")
            print(f"    æ¨¡å‹embedding: {model_vocab_size}, tokenizer: {tokenizer_vocab_size} (å·®å¼‚: +{size_diff})")
            print(f"    è¿™æ˜¯å®‰å…¨çš„é…ç½®ï¼Œembeddingå±‚å¤§äºtokenizerå¯ä»¥é¿å…ç´¢å¼•è¶Šç•Œ")
    else:
        size_diff = tokenizer_vocab_size - model_vocab_size
        print(f"âŒ æ¨¡å‹embeddingå±‚å¤§å°({model_vocab_size})å°äºtokenizerè¯æ±‡è¡¨å¤§å°({tokenizer_vocab_size})")
        print(f"    å·®å¼‚: -{size_diff}ï¼Œè¿™ä¼šå¯¼è‡´è®­ç»ƒæ—¶ç´¢å¼•è¶Šç•Œé”™è¯¯")
        success = False

    # 3. æ£€æŸ¥pad tokenè®¾ç½®
    if tokenizer.pad_token is not None:
        print(f"âœ… pad_tokenå·²è®¾ç½®: {tokenizer.pad_token}")
    else:
        print("âš ï¸  pad_tokenæœªè®¾ç½®")

    return success


def verify_special_tokens_in_data(dataset, tokenizer, sample_count=10):
    """
    éªŒè¯æ•°æ®é¢„å¤„ç†åæ˜¯å¦åŒ…å«ç‰¹æ®Štoken
    """
    print(f"\n{'='*50}")
    print("éªŒè¯æ•°æ®ä¸­çš„ç‰¹æ®Štoken")
    print(f"{'='*50}")

    # è·å–ç‰¹æ®Štokençš„ID
    expand_token_id = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS['expand'])
    enough_token_id = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS['enough'])

    expand_count = 0
    enough_count = 0
    mixed_count = 0  # åŒæ—¶åŒ…å«ä¸¤ç§tokençš„æ ·æœ¬ï¼ˆå¼‚å¸¸æƒ…å†µï¼‰
    total_samples = min(sample_count, len(dataset))

    # é•¿åº¦åˆ†å¸ƒç»Ÿè®¡
    length_distribution = {
        'short_with_enough': 0,    # â‰¤64 tokensä¸”åŒ…å«<|enough|>
        'short_with_expand': 0,    # â‰¤64 tokensä½†åŒ…å«<|expand|>ï¼ˆå¼‚å¸¸ï¼‰
        'long_with_expand': 0,     # >64 tokensä¸”åŒ…å«<|expand|>
        'long_with_enough': 0,     # >64 tokensä½†åŒ…å«<|enough|>ï¼ˆå¼‚å¸¸ï¼‰
        'no_special_tokens': 0     # æ— ç‰¹æ®Štoken
    }

    position_accuracy = {
        'correct_positions': 0,
        'incorrect_positions': 0,
        'position_details': []
    }

    for i in range(total_samples):
        sample = dataset[i]
        input_ids = sample['input_ids']

        # è®¡ç®—prompté•¿åº¦å’Œresponseé•¿åº¦
        prompt_length = sample.get('prompt_lengths', 0)
        if isinstance(prompt_length, torch.Tensor):
            prompt_length = prompt_length.item()

        # ä¼°ç®—responseé•¿åº¦ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        total_length = len(input_ids)
        estimated_response_length = total_length - prompt_length

        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹æ®Štoken
        has_expand = (input_ids == expand_token_id).any().item()
        has_enough = (input_ids == enough_token_id).any().item()

        # åˆ†ç±»ç»Ÿè®¡
        if has_expand and has_enough:
            mixed_count += 1
            print(f"âš ï¸  æ ·æœ¬ {i+1}: å¼‚å¸¸ - åŒæ—¶åŒ…å«ä¸¤ç§ç‰¹æ®Štoken")
        elif has_expand:
            expand_count += 1
            expand_positions = (input_ids == expand_token_id).nonzero(as_tuple=True)[0].tolist()
            relative_positions = [pos - prompt_length for pos in expand_positions]

            # éªŒè¯ä½ç½®ç²¾ç¡®åº¦
            expected_positions = [63, 127, 255, 511, 1023]
            correct_positions = [pos for pos in relative_positions if pos in expected_positions]
            incorrect_positions = [pos for pos in relative_positions if pos not in expected_positions]

            if incorrect_positions:
                position_accuracy['incorrect_positions'] += 1
                position_accuracy['position_details'].append({
                    'sample_id': i + 1,
                    'incorrect_positions': incorrect_positions,
                    'all_positions': relative_positions
                })
                print(f"âŒ æ ·æœ¬ {i+1}: <|expand|> ä½ç½®é”™è¯¯ - ç›¸å¯¹ä½ç½®: {relative_positions}")
            else:
                position_accuracy['correct_positions'] += 1
                print(f"âœ… æ ·æœ¬ {i+1}: <|expand|> ä½ç½®æ­£ç¡® - ç›¸å¯¹ä½ç½®: {relative_positions}")

            # é•¿åº¦åˆ†å¸ƒç»Ÿè®¡
            if estimated_response_length <= 64:
                length_distribution['short_with_expand'] += 1
                print(f"  âš ï¸  çŸ­å›ç­”({estimated_response_length} tokens)å´ä½¿ç”¨<|expand|>")
            else:
                length_distribution['long_with_expand'] += 1
                print(f"  âœ… é•¿å›ç­”({estimated_response_length} tokens)æ­£ç¡®ä½¿ç”¨<|expand|>")

        elif has_enough:
            enough_count += 1
            enough_positions = (input_ids == enough_token_id).nonzero(as_tuple=True)[0].tolist()
            relative_positions = [pos - prompt_length for pos in enough_positions]

            print(f"æ ·æœ¬ {i+1}: åŒ…å« <|enough|> tokenï¼Œç›¸å¯¹ä½ç½®: {relative_positions}")

            # é•¿åº¦åˆ†å¸ƒç»Ÿè®¡
            if estimated_response_length <= 64:
                length_distribution['short_with_enough'] += 1
                print(f"  âœ… çŸ­å›ç­”({estimated_response_length} tokens)æ­£ç¡®ä½¿ç”¨<|enough|>")
            else:
                length_distribution['long_with_enough'] += 1
                print(f"  âš ï¸  é•¿å›ç­”({estimated_response_length} tokens)å´ä½¿ç”¨<|enough|>")
        else:
            length_distribution['no_special_tokens'] += 1
            print(f"æ ·æœ¬ {i+1}: æœªåŒ…å«ç‰¹æ®Štoken (responseé•¿åº¦: ~{estimated_response_length})")

    # æ‰“å°è¯¦ç»†ç»Ÿè®¡
    print(f"\nğŸ“Š è¯¦ç»†ç»Ÿè®¡ç»“æœ (æ£€æŸ¥äº† {total_samples} ä¸ªæ ·æœ¬):")
    print(f"åŒ…å« <|expand|> tokençš„æ ·æœ¬: {expand_count}")
    print(f"åŒ…å« <|enough|> tokençš„æ ·æœ¬: {enough_count}")
    print(f"åŒæ—¶åŒ…å«ä¸¤ç§tokençš„æ ·æœ¬: {mixed_count}")
    print(f"æœªåŒ…å«ç‰¹æ®Štokençš„æ ·æœ¬: {length_distribution['no_special_tokens']}")

    print(f"\nğŸ“ é•¿åº¦åˆ†å¸ƒéªŒè¯:")
    print(f"çŸ­å›ç­”æ­£ç¡®ä½¿ç”¨<|enough|>: {length_distribution['short_with_enough']}")
    print(f"çŸ­å›ç­”é”™è¯¯ä½¿ç”¨<|expand|>: {length_distribution['short_with_expand']}")
    print(f"é•¿å›ç­”æ­£ç¡®ä½¿ç”¨<|expand|>: {length_distribution['long_with_expand']}")
    print(f"é•¿å›ç­”é”™è¯¯ä½¿ç”¨<|enough|>: {length_distribution['long_with_enough']}")

    print(f"\nğŸ¯ ä½ç½®ç²¾ç¡®åº¦:")
    print(f"ä½ç½®æ­£ç¡®çš„æ ·æœ¬: {position_accuracy['correct_positions']}")
    print(f"ä½ç½®é”™è¯¯çš„æ ·æœ¬: {position_accuracy['incorrect_positions']}")

    if position_accuracy['position_details']:
        print(f"\nâŒ ä½ç½®é”™è¯¯è¯¦æƒ…:")
        for detail in position_accuracy['position_details']:
            print(f"  æ ·æœ¬ {detail['sample_id']}: é”™è¯¯ä½ç½® {detail['incorrect_positions']}")

    return {
        'total_samples': total_samples,
        'expand_count': expand_count,
        'enough_count': enough_count,
        'mixed_count': mixed_count,
        'no_special_tokens': length_distribution['no_special_tokens'],
        'length_distribution': length_distribution,
        'position_accuracy': position_accuracy
    }


def verify_special_token_positions(dataset, tokenizer, sample_count=10):
    """
    éªŒè¯ç‰¹æ®Štokençš„ç²¾ç¡®ä½ç½®
    """
    print(f"\n{'='*60}")
    print("ğŸ¯ ç‰¹æ®Štokenä½ç½®ç²¾ç¡®éªŒè¯")
    print(f"{'='*60}")

    # è·å–ç‰¹æ®Štokençš„ID
    expand_token_id = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS['expand'])
    enough_token_id = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS['enough'])

    # é¢„æœŸçš„expand tokenä½ç½®ï¼ˆç›¸å¯¹äºresponseèµ·å§‹çš„ä½ç½®ï¼‰
    expected_expand_positions = [63, 127, 255, 511, 1023]

    # éªŒè¯ç»“æœç»Ÿè®¡
    results = {
        'total_samples': 0,
        'expand_samples': 0,
        'enough_samples': 0,
        'position_correct': 0,
        'position_errors': [],
        'expand_position_stats': {pos: 0 for pos in expected_expand_positions},
        'enough_position_correct': 0,
        'detailed_results': []
    }

    total_samples = min(sample_count, len(dataset))
    results['total_samples'] = total_samples

    for i in range(total_samples):
        sample = dataset[i]
        input_ids = sample['input_ids']

        # è®¡ç®—prompté•¿åº¦ï¼ˆç”¨äºç¡®å®šresponseèµ·å§‹ä½ç½®ï¼‰
        prompt_length = sample.get('prompt_lengths', 0)
        if isinstance(prompt_length, torch.Tensor):
            prompt_length = prompt_length.item()

        # æ‰¾åˆ°ç‰¹æ®Štokençš„ä½ç½®
        expand_positions = (input_ids == expand_token_id).nonzero(as_tuple=True)[0].tolist()
        enough_positions = (input_ids == enough_token_id).nonzero(as_tuple=True)[0].tolist()

        sample_result = {
            'sample_id': i + 1,
            'prompt_length': prompt_length,
            'expand_positions': expand_positions,
            'enough_positions': enough_positions,
            'expand_relative_positions': [],
            'enough_relative_positions': [],
            'position_correct': True,
            'errors': []
        }

        # è®¡ç®—ç›¸å¯¹äºresponseèµ·å§‹çš„ä½ç½®
        if expand_positions:
            sample_result['expand_relative_positions'] = [pos - prompt_length for pos in expand_positions]
            results['expand_samples'] += 1

            # éªŒè¯expand tokenä½ç½®
            relative_positions = sample_result['expand_relative_positions']
            position_errors = []

            for rel_pos in relative_positions:
                if rel_pos in expected_expand_positions:
                    results['expand_position_stats'][rel_pos] += 1
                else:
                    position_errors.append(f"æ„å¤–ä½ç½®: {rel_pos}")

            # æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„é¢„æœŸä½ç½®
            for expected_pos in expected_expand_positions:
                if expected_pos not in relative_positions:
                    # éœ€è¦æ£€æŸ¥åŸå§‹responseé•¿åº¦æ¥åˆ¤æ–­æ˜¯å¦åº”è¯¥æœ‰è¿™ä¸ªä½ç½®
                    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåªè®°å½•å®é™…æ‰¾åˆ°çš„ä½ç½®
                    pass

            if position_errors:
                sample_result['position_correct'] = False
                sample_result['errors'].extend(position_errors)
                results['position_errors'].append({
                    'sample_id': i + 1,
                    'errors': position_errors,
                    'found_positions': relative_positions,
                    'expected_positions': expected_expand_positions
                })

        if enough_positions:
            sample_result['enough_relative_positions'] = [pos - prompt_length for pos in enough_positions]
            results['enough_samples'] += 1

            # éªŒè¯enough tokenä½ç½®ï¼ˆåº”è¯¥åœ¨æœ«å°¾ï¼‰
            # ç®€åŒ–éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦åªæœ‰ä¸€ä¸ªenough token
            if len(enough_positions) == 1:
                results['enough_position_correct'] += 1
            else:
                sample_result['position_correct'] = False
                sample_result['errors'].append(f"enough tokenæ•°é‡å¼‚å¸¸: {len(enough_positions)}")

        if sample_result['position_correct']:
            results['position_correct'] += 1

        results['detailed_results'].append(sample_result)

        # æ‰“å°è¯¦ç»†ä¿¡æ¯
        print(f"\næ ·æœ¬ {i+1}:")
        print(f"  Prompté•¿åº¦: {prompt_length}")

        if expand_positions:
            print(f"  <|expand|> ç»å¯¹ä½ç½®: {expand_positions}")
            print(f"  <|expand|> ç›¸å¯¹ä½ç½®: {sample_result['expand_relative_positions']}")

            # éªŒè¯æ¯ä¸ªä½ç½®
            for rel_pos in sample_result['expand_relative_positions']:
                if rel_pos in expected_expand_positions:
                    print(f"    âœ… ä½ç½® {rel_pos} æ­£ç¡®")
                else:
                    print(f"    âŒ ä½ç½® {rel_pos} é”™è¯¯ï¼ˆé¢„æœŸ: {expected_expand_positions}ï¼‰")

        if enough_positions:
            print(f"  <|enough|> ç»å¯¹ä½ç½®: {enough_positions}")
            print(f"  <|enough|> ç›¸å¯¹ä½ç½®: {sample_result['enough_relative_positions']}")
            if len(enough_positions) == 1:
                print(f"    âœ… <|enough|> tokenä½ç½®æ­£ç¡®")
            else:
                print(f"    âŒ <|enough|> tokenæ•°é‡å¼‚å¸¸: {len(enough_positions)}")

        if not expand_positions and not enough_positions:
            print(f"  âš ï¸  æœªæ‰¾åˆ°ç‰¹æ®Štoken")

    return results


def check_position_consistency(position_results):
    """
    æ£€æŸ¥ä½ç½®ä¸€è‡´æ€§å¹¶æä¾›è¯¦ç»†åˆ†æ

    Args:
        position_results: verify_special_token_positionsçš„è¿”å›ç»“æœ

    Returns:
        dict: ä¸€è‡´æ€§æ£€æŸ¥æŠ¥å‘Š
    """
    print(f"\n{'='*60}")
    print("ğŸ“Š ä½ç½®ä¸€è‡´æ€§åˆ†æ")
    print(f"{'='*60}")

    total_samples = position_results['total_samples']
    position_correct = position_results['position_correct']

    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = (position_correct / total_samples * 100) if total_samples > 0 else 0

    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"ä½ç½®æ­£ç¡®æ ·æœ¬æ•°: {position_correct}")
    print(f"ä½ç½®å‡†ç¡®ç‡: {accuracy:.1f}%")

    # åˆ†æexpand tokenä½ç½®åˆ†å¸ƒ
    print(f"\n<|expand|> tokenä½ç½®åˆ†å¸ƒ:")
    expected_positions = [63, 127, 255, 511, 1023]
    for pos in expected_positions:
        count = position_results['expand_position_stats'][pos]
        print(f"  ä½ç½® {pos}: {count} æ¬¡")

    # åˆ†æé”™è¯¯æƒ…å†µ
    if position_results['position_errors']:
        print(f"\nâŒ å‘ç° {len(position_results['position_errors'])} ä¸ªä½ç½®é”™è¯¯:")
        for error in position_results['position_errors']:
            print(f"  æ ·æœ¬ {error['sample_id']}:")
            print(f"    é”™è¯¯: {error['errors']}")
            print(f"    å®é™…ä½ç½®: {error['found_positions']}")
            print(f"    é¢„æœŸä½ç½®: {error['expected_positions']}")

    # æä¾›ä¿®å¤å»ºè®®
    if accuracy < 100:
        print(f"\nğŸ”§ ä¿®å¤å»ºè®®:")
        print(f"  1. æ£€æŸ¥data_process.pyä¸­çš„ç‰¹æ®Štokenæ’å…¥é€»è¾‘")
        print(f"  2. éªŒè¯target_positions = [63, 127, 255, 511, 1023]è®¾ç½®")
        print(f"  3. ç¡®è®¤response_start_idxè®¡ç®—æ˜¯å¦æ­£ç¡®")
        print(f"  4. æ£€æŸ¥æ˜¯å¦æœ‰ä½ç½®åç§»é—®é¢˜")

    consistency_report = {
        'accuracy': accuracy,
        'total_samples': total_samples,
        'correct_samples': position_correct,
        'error_count': len(position_results['position_errors']),
        'expand_distribution': position_results['expand_position_stats'],
        'enough_correct': position_results['enough_position_correct'],
        'recommendations': []
    }

    if accuracy < 100:
        consistency_report['recommendations'] = [
            "æ£€æŸ¥ç‰¹æ®Štokenæ’å…¥é€»è¾‘",
            "éªŒè¯ä½ç½®è®¡ç®—ç®—æ³•",
            "ç¡®è®¤responseèµ·å§‹ä½ç½®è®¡ç®—"
        ]

    return consistency_report



def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•°æ®é›†åŠ è½½å’Œé¢„å¤„ç†æµ‹è¯•")

    # æ•°æ®ç›¸å…³å‚æ•°
    parser.add_argument("--train_data", type=str, default="simplescaling/s1K",
                       help="è®­ç»ƒæ•°æ®è·¯å¾„ï¼ˆHuggingFaceæ•°æ®é›†åç§°æˆ–æœ¬åœ°JSONæ–‡ä»¶è·¯å¾„ï¼‰")

    # æ¨¡å‹ç›¸å…³å‚æ•°
    parser.add_argument("--model_name", type=str, default="GSAI-ML/LLaDA-8B-Instruct",
                       help="æ¨¡å‹åç§°")
    parser.add_argument("--model_path", type=str, default="GSAI-ML/LLaDA-8B-Instruct",
                       help="æ¨¡å‹è·¯å¾„")

    # å¤„ç†å‚æ•°
    parser.add_argument("--max_length", type=int, default=4096,
                       help="æœ€å¤§åºåˆ—é•¿åº¦")

    args = parser.parse_args()

    try:
        print("æ­£åœ¨åŠ è½½å®Œæ•´çš„æ¨¡å‹å’Œåˆ†è¯å™¨...")

        # åŠ è½½å®Œæ•´çš„æ¨¡å‹å’Œtokenizerï¼ˆçœŸå®è®­ç»ƒç¯å¢ƒï¼‰
        model_loader = TransformerModelLoader(args.model_name, args.model_path)
        tokenizer, model = model_loader.load_model_tokenizer()
        print("âœ… æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸ")

        print(f"åŸå§‹tokenizerè¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
        print(f"åŸå§‹æ¨¡å‹embeddingå±‚å¤§å°: {model.get_input_embeddings().weight.size(0)}")

        # ä½¿ç”¨çœŸå®çš„ç‰¹æ®Štokenè®¾ç½®æµç¨‹ï¼ˆå®Œå…¨æ¨¡æ‹Ÿsft.pyä¸­çš„å¤„ç†ï¼‰
        print(f"\n{'='*50}")
        print("è®¾ç½®ç‰¹æ®Štokenå¹¶è°ƒæ•´æ¨¡å‹embeddingå±‚")
        print(f"{'='*50}")

        model, tokenizer, tokens_added = setup_model_and_tokenizer_for_special_tokens(model, tokenizer)

        if tokens_added:
            print("âœ… ç‰¹æ®Štokenè®¾ç½®å®Œæˆï¼Œæ¨¡å‹å·²å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒ")
            print(f"è°ƒæ•´åtokenizerè¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
            print(f"è°ƒæ•´åæ¨¡å‹embeddingå±‚å¤§å°: {model.get_input_embeddings().weight.size(0)}")
        else:
            print("â„¹ï¸  ç‰¹æ®Štokenå·²å­˜åœ¨ï¼Œæ— éœ€è°ƒæ•´")

        # éªŒè¯ç‰¹æ®Štokenè®¾ç½®
        setup_success = verify_special_tokens_setup(model, tokenizer)
        if not setup_success:
            print("âŒ ç‰¹æ®Štokenè®¾ç½®éªŒè¯å¤±è´¥")
            return 1

        # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        print(f"\n{'='*50}")
        print("å¼€å§‹æ•°æ®åŠ è½½å’Œé¢„å¤„ç†")
        print(f"{'='*50}")
        train_dataset, eval_dataset = load_data(args, tokenizer)

        # åŸºæœ¬ä¿¡æ¯è¾“å‡º
        print(f"\n{'='*50}")
        print("æ•°æ®åŠ è½½å®Œæˆ")
        print(f"{'='*50}")
        print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(eval_dataset)}")

        # éªŒè¯æ•°æ®ä¸­çš„ç‰¹æ®Štokenï¼ˆåŸºç¡€éªŒè¯ï¼‰
        train_stats = None
        eval_stats = None
        if len(train_dataset) > 0:
            train_stats = verify_special_tokens_in_data(train_dataset, tokenizer, sample_count=10000)

        if len(eval_dataset) > 0:
            eval_stats = verify_special_tokens_in_data(eval_dataset, tokenizer, sample_count=10000)

        # ğŸ¯ ç²¾ç¡®ä½ç½®éªŒè¯æµ‹è¯•
        print(f"\n{'='*60}")
        print("ğŸš€ å¼€å§‹ç²¾ç¡®ä½ç½®éªŒè¯æµ‹è¯•")
        print(f"{'='*60}")

        position_test_results = {}

        if len(train_dataset) > 0:
            print(f"\nğŸ“‹ è®­ç»ƒé›†ä½ç½®éªŒè¯:")
            train_position_results = verify_special_token_positions(train_dataset, tokenizer, sample_count=10000)
            train_consistency = check_position_consistency(train_position_results)
            position_test_results['train'] = {
                'position_results': train_position_results,
                'consistency': train_consistency
            }

        if len(eval_dataset) > 0:
            print(f"\nğŸ“‹ éªŒè¯é›†ä½ç½®éªŒè¯:")
            eval_position_results = verify_special_token_positions(eval_dataset, tokenizer, sample_count=10000)
            eval_consistency = check_position_consistency(eval_position_results)
            position_test_results['eval'] = {
                'position_results': eval_position_results,
                'consistency': eval_consistency
            }



        # ğŸ¯ ç»¼åˆéªŒè¯æ€»ç»“
        print(f"\n{'='*70}")
        print("ğŸ¯ å®Œæ•´è®­ç»ƒç¯å¢ƒæ¨¡æ‹Ÿæµ‹è¯•æ€»ç»“")
        print(f"{'='*70}")

        # åŸºç¡€è®¾ç½®éªŒè¯
        print("ğŸ“‹ åŸºç¡€è®¾ç½®éªŒè¯:")
        print("âœ… æ¨¡å‹å’ŒtokenizeråŠ è½½æˆåŠŸ")
        print("âœ… ç‰¹æ®Štokenè®¾ç½®å’Œæ¨¡å‹embeddingå±‚è°ƒæ•´å®Œæˆ")

        # é‡æ–°éªŒè¯ç‰¹æ®Štokenè®¾ç½®ï¼ˆç”¨äºæœ€ç»ˆæŠ¥å‘Šï¼‰
        final_setup_success = verify_special_tokens_setup(model, tokenizer)
        if final_setup_success:
            print("âœ… ç‰¹æ®Štokenè®¾ç½®éªŒè¯é€šè¿‡")
        else:
            print("âŒ ç‰¹æ®Štokenè®¾ç½®éªŒè¯å¤±è´¥")

        print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        print(f"âœ… è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"âœ… éªŒè¯é›†æ ·æœ¬æ•°: {len(eval_dataset)}")

        # ä½ç½®éªŒè¯æ€»ç»“
        print(f"\nğŸ¯ ç‰¹æ®Štokenä½ç½®éªŒè¯æ€»ç»“:")
        overall_position_accuracy = 0
        total_position_tests = 0

        if 'train' in position_test_results:
            train_accuracy = position_test_results['train']['consistency']['accuracy']
            train_samples = position_test_results['train']['consistency']['total_samples']
            print(f"âœ… è®­ç»ƒé›†ä½ç½®éªŒè¯: {train_accuracy:.1f}% å‡†ç¡®ç‡ ({train_samples} æ ·æœ¬)")
            overall_position_accuracy += train_accuracy * train_samples
            total_position_tests += train_samples

        if 'eval' in position_test_results:
            eval_accuracy = position_test_results['eval']['consistency']['accuracy']
            eval_samples = position_test_results['eval']['consistency']['total_samples']
            print(f"âœ… éªŒè¯é›†ä½ç½®éªŒè¯: {eval_accuracy:.1f}% å‡†ç¡®ç‡ ({eval_samples} æ ·æœ¬)")
            overall_position_accuracy += eval_accuracy * eval_samples
            total_position_tests += eval_samples

        if total_position_tests > 0:
            overall_accuracy = overall_position_accuracy / total_position_tests
            print(f"ğŸ¯ æ€»ä½“ä½ç½®å‡†ç¡®ç‡: {overall_accuracy:.1f}%")

            if overall_accuracy >= 95:
                print("ğŸ‰ ä½ç½®éªŒè¯ä¼˜ç§€ï¼ç‰¹æ®Štokenæ’å…¥ä½ç½®é«˜åº¦å‡†ç¡®")
            elif overall_accuracy >= 80:
                print("âœ… ä½ç½®éªŒè¯è‰¯å¥½ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´")
            else:
                print("âš ï¸  ä½ç½®éªŒè¯éœ€è¦æ”¹è¿›ï¼Œå»ºè®®æ£€æŸ¥æ’å…¥é€»è¾‘")

        # åŠŸèƒ½å®Œæ•´æ€§éªŒè¯
        print(f"\nğŸ“Š åŠŸèƒ½å®Œæ•´æ€§éªŒè¯:")
        if train_stats:
            if train_stats.get('mixed_count', 0) == 0:
                print("âœ… æ— å¼‚å¸¸æ ·æœ¬ï¼ˆåŒæ—¶åŒ…å«ä¸¤ç§ç‰¹æ®Štokenï¼‰")
            else:
                print(f"âš ï¸  å‘ç° {train_stats['mixed_count']} ä¸ªå¼‚å¸¸æ ·æœ¬")

        # æœ€ç»ˆçŠ¶æ€åˆ¤æ–­
        all_tests_passed = True

        # æ£€æŸ¥ç‰¹æ®Štokenè®¾ç½®
        if not final_setup_success:
            all_tests_passed = False

        # æ£€æŸ¥ä½ç½®éªŒè¯å‡†ç¡®ç‡
        if total_position_tests > 0 and overall_accuracy < 80:
            all_tests_passed = False

        print(f"\n{'='*70}")
        if all_tests_passed:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å®Œæ•´è®­ç»ƒç¯å¢ƒæ¨¡æ‹Ÿæµ‹è¯•æˆåŠŸå®Œæˆ!")
            print("   ç‰¹æ®Štokenå¤„ç†æµç¨‹éªŒè¯é€šè¿‡ï¼Œå¯ä»¥å®‰å…¨è¿›è¡Œå®é™…è®­ç»ƒã€‚")
            print("   ä½ç½®æ’å…¥å‡†ç¡®ï¼Œæ•°æ®é¢„å¤„ç†æ­£å¸¸ï¼Œæ¨¡å‹å·²æ­£ç¡®è°ƒæ•´ã€‚")
        else:
            print("âš ï¸  éƒ¨åˆ†æµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼Œå»ºè®®æ£€æŸ¥ä»¥ä¸‹é—®é¢˜:")
            if not final_setup_success:
                print("   - ç‰¹æ®Štokenè®¾ç½®å­˜åœ¨é—®é¢˜")
                print("   - æ£€æŸ¥æ¨¡å‹embeddingå±‚ä¸tokenizerçš„å…¼å®¹æ€§")
            if total_position_tests > 0 and overall_accuracy < 80:
                print("   - ç‰¹æ®Štokenä½ç½®æ’å…¥å‡†ç¡®ç‡åä½")
                print("   - æ£€æŸ¥data_process.pyä¸­çš„æ’å…¥é€»è¾‘")
            print("   å»ºè®®ä¿®å¤é—®é¢˜åé‡æ–°æµ‹è¯•ã€‚")

        # é‡Šæ”¾æ¨¡å‹å†…å­˜
        del model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        print("âœ… æ¨¡å‹å†…å­˜å·²é‡Šæ”¾")

        print(f"{'='*70}")

        # è¿”å›æµ‹è¯•ç»“æœçŠ¶æ€
        return 0 if all_tests_passed else 1

    except Exception as e:
        print(f"\n{'='*70}")
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print(f"{'='*70}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)