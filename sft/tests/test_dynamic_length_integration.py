#!/usr/bin/env python3
"""
åŠ¨æ€é•¿åº¦å¾®è°ƒé›†æˆæµ‹è¯•è„šæœ¬

æµ‹è¯•æ–°é›†æˆçš„åŠ¨æ€é•¿åº¦åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œï¼ŒåŒæ—¶ç¡®ä¿ç°æœ‰åŠŸèƒ½ä¸å—å½±å“ã€‚
"""

import os
import sys
import argparse
import torch
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data.dataloader import dLLMDataCollator
from trainer.dynamic_length_trainer import DynamicLengthTrainer
from trainer.trainer import dLLMTrainer
from argsparser import ArgsProcessor


def test_data_collator():
    """æµ‹è¯•æ•°æ®æ•´ç†å™¨çš„åŠ¨æ€é•¿åº¦åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ•°æ®æ•´ç†å™¨...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„tokenizer
    class MockTokenizer:
        def __init__(self):
            self.mask_token_id = 126336
            self.unk_token_id = 0
            
        def convert_tokens_to_ids(self, token):
            # æ¨¡æ‹Ÿç‰¹æ®Štoken ID
            token_map = {
                "<|expand|>": 126337,
                "<|enough|>": 126338
            }
            return token_map.get(token, self.unk_token_id)
    
    tokenizer = MockTokenizer()
    
    # æµ‹è¯•æ ‡å‡†æ¨¡å¼ï¼ˆåº”è¯¥ä¸ç°æœ‰åŠŸèƒ½å®Œå…¨ç›¸åŒï¼‰
    print("  ğŸ“‹ æµ‹è¯•æ ‡å‡†æ¨¡å¼...")
    standard_collator = dLLMDataCollator(
        tokenizer=tokenizer,
        max_length=512,
        enable_dynamic_length=False
    )
    
    # åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡
    batch = {
        "input_ids": torch.randint(0, 1000, (2, 100)),
        "prompt_lengths": torch.tensor([20, 25])
    }
    
    # æµ‹è¯•æ ‡å‡†å‰å‘è¿‡ç¨‹
    try:
        noisy_batch, t, mask_indices = standard_collator.forward_process(batch.copy())
        print("    âœ… æ ‡å‡†å‰å‘è¿‡ç¨‹æ­£å¸¸")
        print(f"    ğŸ“Š è¾“å‡ºå½¢çŠ¶: noisy_batch={noisy_batch.shape}, t={t.shape}, mask_indices={mask_indices.shape}")
    except Exception as e:
        print(f"    âŒ æ ‡å‡†å‰å‘è¿‡ç¨‹å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•åŠ¨æ€é•¿åº¦æ¨¡å¼
    print("  ğŸ“‹ æµ‹è¯•åŠ¨æ€é•¿åº¦æ¨¡å¼...")
    dynamic_config = {
        'initial_response_length': 64,
        'expansion_steps': [64, 128, 256],
        'confidence_threshold': 0.7
    }
    
    dynamic_collator = dLLMDataCollator(
        tokenizer=tokenizer,
        max_length=512,
        enable_dynamic_length=True,
        dynamic_config=dynamic_config
    )
    
    # æµ‹è¯•åŠ¨æ€å‰å‘è¿‡ç¨‹
    try:
        batch_with_dynamic = batch.copy()
        batch_with_dynamic["current_response_length"] = 64
        
        noisy_batch, t, mask_indices = dynamic_collator.forward_process(batch_with_dynamic)
        print("    âœ… åŠ¨æ€å‰å‘è¿‡ç¨‹æ­£å¸¸")
        print(f"    ğŸ“Š è¾“å‡ºå½¢çŠ¶: noisy_batch={noisy_batch.shape}, t={t.shape}, mask_indices={mask_indices.shape}")
    except Exception as e:
        print(f"    âŒ åŠ¨æ€å‰å‘è¿‡ç¨‹å¤±è´¥: {e}")
        return False
    
    print("âœ… æ•°æ®æ•´ç†å™¨æµ‹è¯•é€šè¿‡")
    return True


def test_trainer_classes():
    """æµ‹è¯•è®­ç»ƒå™¨ç±»çš„å¯¼å…¥å’ŒåŸºæœ¬åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•è®­ç»ƒå™¨ç±»...")

    # æµ‹è¯•ç±»å¯¼å…¥
    print("  ğŸ“‹ æµ‹è¯•ç±»å¯¼å…¥...")
    try:
        from trainer.trainer import dLLMTrainer
        from trainer.dynamic_length_trainer import DynamicLengthTrainer
        print("    âœ… è®­ç»ƒå™¨ç±»å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"    âŒ è®­ç»ƒå™¨ç±»å¯¼å…¥å¤±è´¥: {e}")
        return False

    # æµ‹è¯•åŠ¨æ€é•¿åº¦è®­ç»ƒå™¨çš„åŸºæœ¬å±æ€§
    print("  ğŸ“‹ æµ‹è¯•åŠ¨æ€é•¿åº¦è®­ç»ƒå™¨é…ç½®...")
    try:
        # æµ‹è¯•ä¸åŒé…ç½®ä¸‹çš„åˆå§‹åŒ–
        configs = [
            None,  # æ— é…ç½®
            {},    # ç©ºé…ç½®
            {'enable_dynamic_length': False},  # ç¦ç”¨
            {'enable_dynamic_length': True, 'initial_response_length': 64}  # å¯ç”¨
        ]

        for i, config in enumerate(configs):
            # åˆ›å»ºä¸€ä¸ªæœ€å°çš„è®­ç»ƒå™¨å®ä¾‹æ¥æµ‹è¯•é…ç½®å¤„ç†
            class MinimalTrainer(DynamicLengthTrainer):
                def __init__(self, dynamic_config=None):
                    # è·³è¿‡çˆ¶ç±»åˆå§‹åŒ–ï¼Œåªæµ‹è¯•æˆ‘ä»¬çš„é…ç½®é€»è¾‘
                    self.dynamic_config = dynamic_config or {}
                    self.enable_dynamic_length = self.dynamic_config.get('enable_dynamic_length', False)

                    if self.enable_dynamic_length:
                        self._init_dynamic_length_components()

            trainer = MinimalTrainer(config)
            expected_enabled = bool(config and config.get('enable_dynamic_length', False))

            if trainer.enable_dynamic_length == expected_enabled:
                print(f"    âœ… é…ç½® {i+1}: åŠ¨æ€é•¿åº¦={'å¯ç”¨' if expected_enabled else 'ç¦ç”¨'}")
            else:
                print(f"    âŒ é…ç½® {i+1}: æœŸæœ›{'å¯ç”¨' if expected_enabled else 'ç¦ç”¨'}ï¼Œå®é™…{'å¯ç”¨' if trainer.enable_dynamic_length else 'ç¦ç”¨'}")
                return False

    except Exception as e:
        print(f"    âŒ åŠ¨æ€é•¿åº¦è®­ç»ƒå™¨é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False

    print("âœ… è®­ç»ƒå™¨ç±»æµ‹è¯•é€šè¿‡")
    return True


def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("ğŸ§ª æµ‹è¯•é…ç½®åŠ è½½...")
    
    try:
        # æµ‹è¯•åŠ è½½é»˜è®¤é…ç½®
        config_path = "./config/sft/default_config.yaml"
        if os.path.exists(config_path):
            args_processor = ArgsProcessor(config_path)
            args = argparse.Namespace()
            args = args_processor.add_args_from_yaml(args)
            
            # æ£€æŸ¥åŠ¨æ€é•¿åº¦é…ç½®æ˜¯å¦æ­£ç¡®åŠ è½½
            has_dynamic_config = hasattr(args, 'enable_dynamic_length')
            print(f"    ğŸ“‹ åŠ¨æ€é•¿åº¦é…ç½®å­˜åœ¨: {has_dynamic_config}")
            
            if has_dynamic_config:
                print(f"    ğŸ“Š å¯ç”¨åŠ¨æ€é•¿åº¦: {args.enable_dynamic_length}")
                if hasattr(args, 'dynamic_length'):
                    print(f"    ğŸ“Š åŠ¨æ€é•¿åº¦è¯¦ç»†é…ç½®: {type(args.dynamic_length)}")
            
            print("    âœ… é…ç½®åŠ è½½æˆåŠŸ")
        else:
            print(f"    âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            
    except Exception as e:
        print(f"    âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False
    
    print("âœ… é…ç½®æµ‹è¯•é€šè¿‡")
    return True


def test_backward_compatibility():
    """æµ‹è¯•å‘åå…¼å®¹æ€§"""
    print("ğŸ§ª æµ‹è¯•å‘åå…¼å®¹æ€§...")
    
    # æ¨¡æ‹Ÿæ²¡æœ‰åŠ¨æ€é•¿åº¦é…ç½®çš„æƒ…å†µ
    class OldStyleArgs:
        def __init__(self):
            self.model_name = "test-model"
            self.local_batch_size = 1
            self.max_length = 512
            # æ•…æ„ä¸åŒ…å«åŠ¨æ€é•¿åº¦ç›¸å…³çš„é…ç½®
    
    args = OldStyleArgs()
    
    # æµ‹è¯•æ˜¯å¦èƒ½æ­£å¸¸å¤„ç†ç¼ºå°‘åŠ¨æ€é•¿åº¦é…ç½®çš„æƒ…å†µ
    try:
        enable_dynamic_length = getattr(args, 'enable_dynamic_length', False)
        dynamic_config = getattr(args, 'dynamic_length', None) if enable_dynamic_length else None
        
        print(f"    ğŸ“‹ åŠ¨æ€é•¿åº¦åŠŸèƒ½: {'å¯ç”¨' if enable_dynamic_length else 'ç¦ç”¨'}")
        print(f"    ğŸ“‹ åŠ¨æ€é…ç½®: {dynamic_config}")
        
        # åº”è¯¥é»˜è®¤ç¦ç”¨åŠ¨æ€é•¿åº¦åŠŸèƒ½
        if not enable_dynamic_length and dynamic_config is None:
            print("    âœ… å‘åå…¼å®¹æ€§æ­£å¸¸ - é»˜è®¤ç¦ç”¨åŠ¨æ€é•¿åº¦åŠŸèƒ½")
        else:
            print("    âŒ å‘åå…¼å®¹æ€§é—®é¢˜ - åº”è¯¥é»˜è®¤ç¦ç”¨åŠ¨æ€é•¿åº¦åŠŸèƒ½")
            return False
            
    except Exception as e:
        print(f"    âŒ å‘åå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print("âœ… å‘åå…¼å®¹æ€§æµ‹è¯•é€šè¿‡")
    return True


def test_stage2_features():
    """æµ‹è¯•é˜¶æ®µ2çš„æ ¸å¿ƒåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•é˜¶æ®µ2æ ¸å¿ƒåŠŸèƒ½...")

    # æµ‹è¯•æ¸è¿›å¼é•¿åº¦æ‰©å±•é€»è¾‘
    print("  ğŸ“‹ æµ‹è¯•æ¸è¿›å¼é•¿åº¦æ‰©å±•...")
    try:
        from trainer.dynamic_length_trainer import DynamicLengthTrainer

        # åˆ›å»ºæœ€å°çš„è®­ç»ƒå™¨å®ä¾‹æ¥æµ‹è¯•æ‰©å±•é€»è¾‘
        class TestTrainer(DynamicLengthTrainer):
            def __init__(self, dynamic_config=None):
                self.dynamic_config = dynamic_config or {}
                self.enable_dynamic_length = self.dynamic_config.get('enable_dynamic_length', False)
                self.special_token_ids = {'expand': 126337, 'enough': 126338}

                # æ¨¡æ‹Ÿè®­ç»ƒçŠ¶æ€
                self.state = type('State', (), {'global_step': 150})()

        config = {
            'enable_dynamic_length': True,
            'expansion_steps': [64, 128, 256, 512],
            'initial_response_length': 64,
            'min_steps_per_stage': 100
        }

        trainer = TestTrainer(config)

        # æµ‹è¯•è®­ç»ƒé˜¶æ®µåˆ¤æ–­
        stage_64 = trainer._get_current_training_stage(64, [64, 128, 256, 512])
        stage_128 = trainer._get_current_training_stage(128, [64, 128, 256, 512])
        stage_256 = trainer._get_current_training_stage(256, [64, 128, 256, 512])

        if stage_64 == 0 and stage_128 == 1 and stage_256 == 2:
            print("    âœ… è®­ç»ƒé˜¶æ®µåˆ¤æ–­æ­£ç¡®")
        else:
            print(f"    âŒ è®­ç»ƒé˜¶æ®µåˆ¤æ–­é”™è¯¯: {stage_64}, {stage_128}, {stage_256}")
            return False

        # æµ‹è¯•é•¿åº¦è‡ªé€‚åº”æƒé‡
        weight_64 = trainer._get_length_adaptive_weight(64)
        weight_128 = trainer._get_length_adaptive_weight(128)
        weight_512 = trainer._get_length_adaptive_weight(512)

        if 1.0 <= weight_64 <= 1.5 and 0.8 <= weight_128 <= 1.2 and 0.8 <= weight_512 <= 1.1:
            print("    âœ… é•¿åº¦è‡ªé€‚åº”æƒé‡æ­£ç¡®")
        else:
            print(f"    âŒ é•¿åº¦è‡ªé€‚åº”æƒé‡å¼‚å¸¸: {weight_64}, {weight_128}, {weight_512}")
            return False

    except Exception as e:
        print(f"    âŒ æ¸è¿›å¼é•¿åº¦æ‰©å±•æµ‹è¯•å¤±è´¥: {e}")
        return False

    # æµ‹è¯•ç‰¹æ®Štokenæ£€æµ‹
    print("  ğŸ“‹ æµ‹è¯•ç‰¹æ®Štokenæ£€æµ‹...")
    try:
        # æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
        input_ids = torch.tensor([[1, 2, 3, 126337, 5, 6, 7, 126338, 9, 10]])  # expand at pos 3, enough at pos 7

        # æµ‹è¯•ç›´æ¥è§‚å¯Ÿ
        direct_result = trainer._check_direct_token_observation(
            input_ids[0], 3, 126337, 126338
        )

        if direct_result['should_expand'] and direct_result['token_type'] == 'expand':
            print("    âœ… ç›´æ¥è§‚å¯Ÿexpand tokenæ­£ç¡®")
        else:
            print(f"    âŒ ç›´æ¥è§‚å¯Ÿexpand tokenå¤±è´¥: {direct_result}")
            return False

        direct_result_enough = trainer._check_direct_token_observation(
            input_ids[0], 7, 126337, 126338
        )

        if not direct_result_enough['should_expand'] and direct_result_enough['token_type'] == 'enough':
            print("    âœ… ç›´æ¥è§‚å¯Ÿenough tokenæ­£ç¡®")
        else:
            print(f"    âŒ ç›´æ¥è§‚å¯Ÿenough tokenå¤±è´¥: {direct_result_enough}")
            return False

    except Exception as e:
        print(f"    âŒ ç‰¹æ®Štokenæ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
        return False

    print("âœ… é˜¶æ®µ2æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡")
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åŠ¨æ€é•¿åº¦å¾®è°ƒé›†æˆæµ‹è¯• - é˜¶æ®µ2")
    print("=" * 60)

    tests = [
        ("å‘åå…¼å®¹æ€§", test_backward_compatibility),
        ("é…ç½®åŠ è½½", test_config_loading),
        ("æ•°æ®æ•´ç†å™¨", test_data_collator),
        ("è®­ç»ƒå™¨ç±»", test_trainer_classes),
        ("é˜¶æ®µ2æ ¸å¿ƒåŠŸèƒ½", test_stage2_features),
    ]

    passed = 0
    total = len(tests)

    for test_name, test_func in tests:
        print(f"\nğŸ“ è¿è¡Œæµ‹è¯•: {test_name}")
        print("-" * 40)

        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")

    print("\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åŠ¨æ€é•¿åº¦å¾®è°ƒé˜¶æ®µ2é›†æˆæˆåŠŸï¼")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é—®é¢˜")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
