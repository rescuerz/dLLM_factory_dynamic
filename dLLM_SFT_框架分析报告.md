# dLLM-Factory SFTæ¡†æ¶è¯¦ç»†åˆ†ææŠ¥å‘Š

## æ¦‚è¿°

æœ¬æŠ¥å‘Šè¯¦ç»†åˆ†æäº† `/mnt/40t/zhounan/dLLM-Factory-main/sft/` ç›®å½•çš„ç»“æ„å’ŒåŠŸèƒ½ï¼Œä¸ºç”¨æˆ·æä¾›å…¨é¢çš„å¾®è°ƒæ¡†æ¶ä½¿ç”¨æŒ‡å—ã€‚è¯¥æ¡†æ¶åŸºäºæ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelï¼‰çš„è¯­è¨€æ¨¡å‹å¾®è°ƒæ–¹æ³•ï¼Œæä¾›äº†å®Œæ•´çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è§£å†³æ–¹æ¡ˆã€‚

---

## 1. ç›®å½•ç»“æ„æ¦‚è§ˆ

```
sft/
â”œâ”€â”€ sft.py                    # ä¸»è®­ç»ƒè„šæœ¬å…¥å£
â”œâ”€â”€ argsparser/              # å‚æ•°è§£ææ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ argsparser.py        # YAMLé…ç½®æ–‡ä»¶è§£æå™¨
â”œâ”€â”€ config/                  # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ CONFIG.md           # é…ç½®è¯´æ˜æ–‡æ¡£
â”‚   â”œâ”€â”€ accelerate/         # åˆ†å¸ƒå¼è®­ç»ƒé…ç½®
â”‚   â”‚   â”œâ”€â”€ full_param_config.yaml
â”‚   â”‚   â””â”€â”€ lora_config.yaml
â”‚   â”œâ”€â”€ lora/               # LoRAå¾®è°ƒé…ç½®
â”‚   â”‚   â””â”€â”€ default_config.yaml
â”‚   â””â”€â”€ sft/                # SFTè®­ç»ƒé…ç½®
â”‚       â””â”€â”€ default_config.yaml
â”œâ”€â”€ data/                   # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataloader.py       # æ•°æ®åŠ è½½å™¨å’Œæ•°æ®æ•´ç†å™¨
â”‚   â”œâ”€â”€ dataset.py          # æ•°æ®é›†ç±»å®šä¹‰
â”‚   â””â”€â”€ data_process/       # æ•°æ®é¢„å¤„ç†
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ data_process.py
â”œâ”€â”€ trainer/                # è®­ç»ƒå™¨æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ trainer.py          # è‡ªå®šä¹‰è®­ç»ƒå™¨
â”œâ”€â”€ utils/                  # å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ lora_builder/       # LoRAæ„å»ºå™¨
â”‚   â”œâ”€â”€ model_loader/       # æ¨¡å‹åŠ è½½å™¨
â”‚   â”œâ”€â”€ rl/                 # å¼ºåŒ–å­¦ä¹ ç›¸å…³
â”‚   â””â”€â”€ sampling/           # é‡‡æ ·ç›¸å…³
â””â”€â”€ sft_save/               # æ¨¡å‹ä¿å­˜ç›®å½•
    â””â”€â”€ llada-sft/
```

---

## 2. å„å­ç›®å½•åŠŸèƒ½è¯¦è§£

### ğŸ”§ **config/ - é…ç½®ç®¡ç†ä¸­å¿ƒ**
- **ä½œç”¨**ï¼šé›†ä¸­ç®¡ç†æ‰€æœ‰è®­ç»ƒé…ç½®ï¼Œæ”¯æŒä¸åŒçš„è®­ç»ƒç­–ç•¥
- **æ ¸å¿ƒæ–‡ä»¶**ï¼š
  - `sft/default_config.yaml`ï¼šSFTè®­ç»ƒçš„é»˜è®¤é…ç½®ï¼ŒåŒ…å«æ¨¡å‹åç§°ã€æ‰¹æ¬¡å¤§å°ã€å­¦ä¹ ç‡ç­‰å…³é”®å‚æ•°
  - `accelerate/`ï¼šåˆ†å¸ƒå¼è®­ç»ƒé…ç½®ï¼Œæ”¯æŒDeepSpeedå’Œå¤šGPUè®­ç»ƒ
  - `lora/default_config.yaml`ï¼šLoRAå‚æ•°é«˜æ•ˆå¾®è°ƒé…ç½®
- **ç‰¹ç‚¹**ï¼šä½¿ç”¨YAMLæ ¼å¼ï¼Œä¾¿äºä¿®æ”¹å’Œç‰ˆæœ¬æ§åˆ¶

### ğŸ“Š **data/ - æ•°æ®å¤„ç†æ ¸å¿ƒ**
- **ä½œç”¨**ï¼šå¤„ç†è®­ç»ƒæ•°æ®çš„åŠ è½½ã€é¢„å¤„ç†å’Œæ‰¹æ¬¡æ•´ç†
- **å…³é”®ç»„ä»¶**ï¼š
  - `dLLMSFTDataset`ï¼šä¸“é—¨ä¸ºdLLMæ¨¡å‹è®¾è®¡çš„æ•°æ®é›†ç±»
  - `dLLMDataCollator`ï¼šå®ç°å‰å‘å™ªå£°è¿‡ç¨‹çš„æ•°æ®æ•´ç†å™¨ï¼Œè¿™æ˜¯dLLMçš„æ ¸å¿ƒç‰¹æ€§
  - `data_process/`ï¼šæ•°æ®é¢„å¤„ç†æ¨¡å—ï¼Œè´Ÿè´£æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–
- **ç‰¹è‰²åŠŸèƒ½**ï¼šæ”¯æŒåŠ¨æ€å™ªå£°æ·»åŠ ï¼Œå®ç°diffusion-basedçš„è¯­è¨€æ¨¡å‹è®­ç»ƒ

### ğŸ‹ï¸ **trainer/ - è®­ç»ƒå¼•æ“**
- **ä½œç”¨**ï¼šè‡ªå®šä¹‰è®­ç»ƒé€»è¾‘ï¼Œå®ç°dLLMç‰¹æœ‰çš„æŸå¤±è®¡ç®—
- **æ ¸å¿ƒç‰¹æ€§**ï¼š
  - ç»§æ‰¿è‡ªHuggingFace Trainerï¼Œæ·»åŠ äº†absorbing state diffusion loss
  - æ”¯æŒæ—¶é—´æ­¥é•¿tçš„åŠ¨æ€è°ƒæ•´
  - å®ç°äº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„ç‰¹æ®ŠæŸå¤±è®¡ç®—

### ğŸ› ï¸ **utils/ - å·¥å…·é›†åˆ**
- **model_loader/**ï¼šæ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½å™¨
  - æ”¯æŒHuggingFace Transformersæ¨¡å‹
  - è‡ªåŠ¨å¤„ç†æ¨¡å‹é…ç½®å’Œæƒé‡åŠ è½½
- **lora_builder/**ï¼šLoRAé€‚é…å™¨æ„å»ºå·¥å…·
- **sampling/**ï¼šé‡‡æ ·ç­–ç•¥å®ç°
- **rl/**ï¼šå¼ºåŒ–å­¦ä¹ ç›¸å…³å·¥å…·

### âš™ï¸ **argsparser/ - å‚æ•°ç®¡ç†**
- **ä½œç”¨**ï¼šç»Ÿä¸€çš„é…ç½®æ–‡ä»¶è§£æå™¨
- **åŠŸèƒ½**ï¼šå°†YAMLé…ç½®æ–‡ä»¶è½¬æ¢ä¸ºå‘½ä»¤è¡Œå‚æ•°æ ¼å¼ï¼Œæ”¯æŒåµŒå¥—é…ç½®å±•å¹³

---

## 3. å…³é”®æ–‡ä»¶è¯´æ˜

### ğŸš€ **sft.py - ä¸»å…¥å£è„šæœ¬**
```python
def load_data(args, tokenizer):
    if args.train_data.endswith('.json'):
        from datasets import Dataset
        import json
        with open(args.train_data, 'r', encoding='utf-8') as f:
            data = json.load(f)
        data = Dataset.from_list(data)
    else:
        data = load_dataset(args.train_data, split="train")

    train_data, eval_data = preprocess_dataset(data, tokenizer, args.max_length)
    train_dataset = dLLMSFTDataset(train_data, tokenizer, args.max_length)
    eval_dataset = dLLMSFTDataset(eval_data, tokenizer, args.max_length, eval=True)
    return train_dataset, eval_dataset
```

### ğŸ“‹ **config/sft/default_config.yaml - æ ¸å¿ƒé…ç½®**
```yaml
model_name: "GSAI-ML/LLaDA-8B-Instruct"    # é¢„è®­ç»ƒæ¨¡å‹åç§°
local_batch_size: 1                        # è®­ç»ƒæ‰¹æ¬¡å¤§å°
max_length: 4096                           # åˆ†è¯æœ€å¤§é•¿åº¦
num_epochs: 20                             # è®­ç»ƒè½®æ•°
learning_rate: 1e-5                        # ä¼˜åŒ–å™¨å­¦ä¹ ç‡
grad_accum_steps: 1                        # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
output_dir: "./sft_save"                   # æ¨¡å‹ä¸æ—¥å¿—ä¿å­˜ç›®å½•
job_name: "llada-sft"                      # ä»»åŠ¡åç§°
train_data: "simplescaling/s1K"            # è®­ç»ƒæ•°æ®è·¯å¾„
```

### ğŸ¯ **trainer/trainer.py - è‡ªå®šä¹‰è®­ç»ƒå™¨**
```python
class dLLMTrainer(Trainer):
    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):
        """
        Absorbing state diffusion loss computation
        """
        labels, t, num_prompt_tokens = inputs.pop("labels"), inputs.pop("t"), inputs.pop("num_prompt_tokens")
        outputs = model(**inputs)
        logits = outputs.logits
        unscaled_loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), labels.view(-1), reduction="none").view(logits.shape[0], -1)
        loss = unscaled_loss / t
        loss = loss.sum() / (inputs["input_ids"].numel() - num_prompt_tokens)
        return loss if not return_outputs else (loss, outputs)
```

### ğŸ“Š **data/dataloader.py - æ•°æ®æ•´ç†å™¨**
```python
class dLLMDataCollator(DefaultDataCollator):
    """
    Adds the forward noising process to the batch.
    Modify forward_process to change the noise schedule
    """
    
    def forward_process(self, batch, eps=1e-3):
        input_ids = batch["input_ids"]
        B, N = input_ids.shape
        if "t" not in batch:
            t = torch.rand((B,), device=input_ids.device)
        else:
            t = batch["t"]
        
        t = (1 - eps) * t + eps
        t = t[:, None].repeat(1, N)
        
        mask_indices = torch.rand((B, N), device=input_ids.device) < t
        noisy_batch = torch.where(mask_indices, self.mask_token_id, input_ids)
        return noisy_batch, t, mask_indices
```

---

## 4. å·¥ä½œæµç¨‹å…³ç³»

```mermaid
graph TD
    A[sft.py ä¸»è„šæœ¬] --> B[ArgsProcessor è§£æé…ç½®]
    B --> C[TransformerModelLoader åŠ è½½æ¨¡å‹]
    B --> D[load_data åŠ è½½æ•°æ®]
    D --> E[dLLMSFTDataset æ•°æ®é›†]
    E --> F[dLLMDataCollator æ•°æ®æ•´ç†]
    C --> G[dLLMTrainer è®­ç»ƒå™¨]
    F --> G
    G --> H[æ¨¡å‹è®­ç»ƒä¸ä¿å­˜]
```

**è°ƒç”¨é¡ºåº**ï¼š
1. `sft.py` è¯»å–é…ç½®æ–‡ä»¶
2. `ArgsProcessor` è§£æYAMLé…ç½®
3. `TransformerModelLoader` åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
4. `load_data` å¤„ç†è®­ç»ƒæ•°æ®
5. `dLLMDataCollator` æ·»åŠ å™ªå£°å¤„ç†
6. `dLLMTrainer` æ‰§è¡Œè®­ç»ƒå¾ªç¯
7. ä¿å­˜è®­ç»ƒç»“æœåˆ° `sft_save/`

---

## 5. å¿«é€Ÿä¸Šæ‰‹æŒ‡å—

### ğŸ¯ **æ–°æ‰‹å…¥é—¨è·¯å¾„**

**ç¬¬ä¸€æ­¥ï¼šäº†è§£é…ç½®**
- é¦–å…ˆé˜…è¯» `config/CONFIG.md` äº†è§£é…ç½®ä½“ç³»
- æŸ¥çœ‹ `config/sft/default_config.yaml` ç†è§£åŸºæœ¬å‚æ•°

**ç¬¬äºŒæ­¥ï¼šå‡†å¤‡æ•°æ®**
- å‡†å¤‡JSONæ ¼å¼çš„è®­ç»ƒæ•°æ®æˆ–ä½¿ç”¨HuggingFaceæ•°æ®é›†
- æ•°æ®æ ¼å¼åº”åŒ…å«è¾“å…¥å’Œç›®æ ‡æ–‡æœ¬

**ç¬¬ä¸‰æ­¥ï¼šä¿®æ”¹é…ç½®**
```yaml
# åœ¨ config/sft/default_config.yaml ä¸­ä¿®æ”¹
model_name: "ä½ çš„æ¨¡å‹è·¯å¾„"
train_data: "ä½ çš„æ•°æ®è·¯å¾„"
output_dir: "./ä½ çš„è¾“å‡ºç›®å½•"
```

**ç¬¬å››æ­¥ï¼šå¯åŠ¨è®­ç»ƒ**
```bash
cd /mnt/40t/zhounan/dLLM-Factory-main/sft
python sft.py --config config/sft/default_config.yaml
```

### ğŸ”§ **å…¸å‹é…ç½®æµç¨‹**

1. **é€‰æ‹©è®­ç»ƒç­–ç•¥**ï¼š
   - å…¨å‚æ•°å¾®è°ƒï¼šä½¿ç”¨ `config/accelerate/full_param_config.yaml`
   - LoRAå¾®è°ƒï¼šä½¿ç”¨ `config/lora/default_config.yaml`

2. **è°ƒæ•´å…³é”®å‚æ•°**ï¼š
   - `local_batch_size`ï¼šæ ¹æ®GPUå†…å­˜è°ƒæ•´
   - `max_length`ï¼šæ ¹æ®æ•°æ®ç‰¹ç‚¹è®¾ç½®
   - `learning_rate`ï¼šé€šå¸¸ä»1e-5å¼€å§‹

3. **é…ç½®åˆ†å¸ƒå¼è®­ç»ƒ**ï¼š
   ```bash
   accelerate config  # é…ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
   accelerate launch sft.py --config your_config.yaml
   ```

### ğŸ“ **å¸¸ç”¨å‘½ä»¤**

```bash
# åŸºç¡€è®­ç»ƒ
python sft.py --config config/sft/default_config.yaml

# ä½¿ç”¨LoRAå¾®è°ƒ
python sft.py --config config/lora/default_config.yaml

# åˆ†å¸ƒå¼è®­ç»ƒ
accelerate launch --config_file config/accelerate/lora_config.yaml sft.py

# ç›‘æ§è®­ç»ƒè¿›åº¦
tensorboard --logdir sft_save/llada-sft/logs
```

### ğŸ›ï¸ **é‡è¦å‚æ•°è¯´æ˜**

| å‚æ•°åç§° | é»˜è®¤å€¼ | è¯´æ˜ | è°ƒä¼˜å»ºè®® |
|---------|--------|------|----------|
| `local_batch_size` | 1 | æ¯ä¸ªè®¾å¤‡çš„æ‰¹æ¬¡å¤§å° | æ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼Œé€šå¸¸1-8 |
| `max_length` | 4096 | æœ€å¤§åºåˆ—é•¿åº¦ | æ ¹æ®æ•°æ®ç‰¹ç‚¹è®¾ç½®ï¼Œå½±å“å†…å­˜ä½¿ç”¨ |
| `learning_rate` | 1e-5 | å­¦ä¹ ç‡ | å¤§æ¨¡å‹é€šå¸¸ç”¨1e-5åˆ°5e-5 |
| `grad_accum_steps` | 1 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° | ç”¨äºå¢åŠ æœ‰æ•ˆæ‰¹æ¬¡å¤§å° |
| `num_epochs` | 20 | è®­ç»ƒè½®æ•° | æ ¹æ®æ•°æ®é‡å’Œæ”¶æ•›æƒ…å†µè°ƒæ•´ |

---

## ğŸ¯ **æ ¸å¿ƒç‰¹è‰²**

è¿™ä¸ªSFTæ¡†æ¶çš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºï¼š

1. **Diffusion-basedè®­ç»ƒ**ï¼šå®ç°äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„è¯­è¨€æ¨¡å‹è®­ç»ƒï¼Œé€šè¿‡å™ªå£°æ³¨å…¥å’Œå»å™ªè¿‡ç¨‹æå‡æ¨¡å‹æ€§èƒ½
2. **çµæ´»çš„é…ç½®ç³»ç»Ÿ**ï¼šæ”¯æŒYAMLé…ç½®æ–‡ä»¶ï¼Œä¾¿äºå®éªŒç®¡ç†å’Œå‚æ•°è°ƒä¼˜
3. **å®Œæ•´çš„å·¥å…·é“¾**ï¼šä»æ•°æ®å¤„ç†åˆ°æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹ï¼Œæ”¯æŒå¤šç§è®­ç»ƒç­–ç•¥
4. **ç”Ÿäº§å°±ç»ª**ï¼šæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒã€æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ç­‰é«˜çº§ç‰¹æ€§

### ğŸ”¬ **æŠ€æœ¯äº®ç‚¹**

- **å™ªå£°è°ƒåº¦**ï¼šé€šè¿‡ `dLLMDataCollator` å®ç°åŠ¨æ€å™ªå£°æ·»åŠ 
- **æŸå¤±è®¡ç®—**ï¼šç‰¹æ®Šçš„absorbing state diffusion loss
- **æ—¶é—´æ­¥é•¿**ï¼šæ”¯æŒè¿ç»­æ—¶é—´æ­¥é•¿tçš„è®­ç»ƒ
- **å‚æ•°é«˜æ•ˆ**ï¼šé›†æˆLoRAç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•

---

## ğŸ“š **è¿›é˜¶ä½¿ç”¨**

### è‡ªå®šä¹‰æ•°æ®æ ¼å¼
```python
# æ•°æ®åº”åŒ…å«ä»¥ä¸‹å­—æ®µ
{
    "input": "ç”¨æˆ·è¾“å…¥æ–‡æœ¬",
    "output": "æœŸæœ›è¾“å‡ºæ–‡æœ¬"
}
```

### ç›‘æ§è®­ç»ƒè¿‡ç¨‹
- ä½¿ç”¨ `wandb` è¿›è¡Œå®éªŒè·Ÿè¸ª
- é…ç½® `report_to: "wandb"` å¯ç”¨
- æŸ¥çœ‹æŸå¤±æ›²çº¿å’Œè®­ç»ƒæŒ‡æ ‡

### æ¨¡å‹è¯„ä¼°
- æ¡†æ¶è‡ªåŠ¨è¿›è¡ŒéªŒè¯é›†è¯„ä¼°
- æ”¯æŒè‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
- å¯é…ç½®è¯„ä¼°é¢‘ç‡å’Œç­–ç•¥

---

## ğŸš€ **æ€»ç»“**

dLLM-Factoryçš„SFTæ¡†æ¶æä¾›äº†ä¸€ä¸ªå®Œæ•´ã€çµæ´»ä¸”é«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒè§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å…¶ç‹¬ç‰¹çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•å’Œå®Œå–„çš„å·¥å…·é“¾ï¼Œç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆå¯ä»¥è½»æ¾è¿›è¡Œå„ç§å¾®è°ƒå®éªŒï¼Œä»å‚æ•°é«˜æ•ˆçš„LoRAå¾®è°ƒåˆ°å…¨å‚æ•°å¾®è°ƒï¼Œéƒ½èƒ½å¾—åˆ°å¾ˆå¥½çš„æ”¯æŒã€‚

è¯¥æ¡†æ¶ç‰¹åˆ«é€‚åˆï¼š
- å¤§è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒç ”ç©¶
- å‚æ•°é«˜æ•ˆå¾®è°ƒå®éªŒ
- åˆ†å¸ƒå¼è®­ç»ƒéƒ¨ç½²
- ç”Ÿäº§ç¯å¢ƒçš„æ¨¡å‹è®­ç»ƒ

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š2025-07-13*  
*æ¡†æ¶ç‰ˆæœ¬ï¼šdLLM-Factory*  
*åˆ†æèŒƒå›´ï¼š/mnt/40t/zhounan/dLLM-Factory-main/sft/*
